{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset file and prepare the vocabulary.\n",
    "\n",
    "words: Final[list[str]] = open(\"names.txt\", 'r').read().splitlines()\n",
    "chars: Final[list[str]] = sorted(list(set(''.join(words))))\n",
    "stoi: Final[dict[str, int]] = {char: i + 1 for i, char in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos: Final[dict[int, str]] = {i: char for char, i in stoi.items()}\n",
    "char_cnt: Final[int] = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and the corresponding labels.\n",
    "\n",
    "def create_dataset(words: list[str], block_size: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    X: list[list[int]] = []\n",
    "    Y: list[int] = []\n",
    "    for word in words:\n",
    "        context: list[int] = [0] * block_size\n",
    "        for ix in word + '.':\n",
    "            X.append(context)\n",
    "            Y.append(stoi[ix])\n",
    "            context = context[1:] + [stoi[ix]]\n",
    "    X_t = torch.tensor(X)\n",
    "    Y_t = torch.tensor(Y)\n",
    "    print(X_t.shape, Y_t.shape)\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "device: Final[torch.device] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset for training, validation, and testing.\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "block_size: Final[int] = 3 # context length\n",
    "train_set_ratio: float = 0.8\n",
    "valid_set_ratio: float = 0.1\n",
    "test_set_ratio: float = 0.1\n",
    "\n",
    "n1: int = int(len(words) * train_set_ratio)\n",
    "n2: int = int(len(words) * (train_set_ratio + valid_set_ratio))\n",
    "\n",
    "X_train, Y_train = create_dataset(words[:n1], block_size)\n",
    "X_val, Y_val = create_dataset(words[n1:n2], block_size)\n",
    "X_test, Y_test = create_dataset(words[n2:], block_size)\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "Y_val = Y_val.to(device)\n",
    "X_test = X_test.to(device)\n",
    "Y_test = Y_test.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gradient checking functions.\n",
    "def cmp_grad(p_name: str, dp: torch.Tensor, p: torch.Tensor) -> None:\n",
    "    assert p.grad is not None, f\"Gradient for {p_name} is None\"\n",
    "    exact_eq: Final[bool] = bool(torch.all(dp == p.grad).item())\n",
    "    apprx_eq: Final[bool] = torch.allclose(dp, p.grad)\n",
    "    max_diff: Final[float] = (dp - p.grad).abs().max().item()\n",
    "    print(f\"{p_name:18s} | shape equal: {dp.shape == p.grad.shape} | \"\n",
    "          f\"exact equal: {str(exact_eq):5s} | \"\n",
    "          f\"approximate equal: {str(apprx_eq):5s} | max_diff: {max_diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 36837\n"
     ]
    }
   ],
   "source": [
    "# With batch normalization, we don't need to initialize the bias, but we\n",
    "# do it for calculating the gradient manually.\n",
    "# zero bias can mask the incorrect calculation of the gradient, so we\n",
    "# initialize it to a small value so that we can still check its value.\n",
    "\n",
    "num_neurons: Final[int] = 300\n",
    "embed_dim: Final[int] = 30\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((char_cnt, embed_dim), generator=g).to(device)\n",
    "# Layer 1\n",
    "W1 = torch.randn((block_size * embed_dim, num_neurons), generator=g).to(device) * (5 / 3) / ((embed_dim * block_size) ** 0.5)\n",
    "b1 = torch.randn(num_neurons, generator=g).to(device) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((num_neurons, char_cnt), generator=g).to(device) * 0.1\n",
    "b2 = torch.randn(char_cnt, generator=g).to(device) * 0.1\n",
    "\n",
    "# Batch normalization parameters\n",
    "bn_gain = torch.randn((1, num_neurons), generator=g).to(device) * 0.1 + 1.0\n",
    "bn_bias = torch.randn((1, num_neurons), generator=g).to(device) * 0.1\n",
    "bn_mean_running = torch.zeros((1, num_neurons)).to(device)\n",
    "bn_std_running = torch.ones((1, num_neurons)).to(device)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(f\"parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_loss(X_t: torch.Tensor, Y_t: torch.Tensor) -> torch.Tensor:\n",
    "    emb = C[X_t]\n",
    "    embcat = emb.view(-1, block_size * embed_dim)\n",
    "    h_preact = embcat @ W1 + b1\n",
    "    # Use the running mean/std.\n",
    "    h_preact_norm = (h_preact - bn_mean_running) / bn_std_running\n",
    "    h_preact = h_preact_norm * bn_gain + bn_bias\n",
    "    h = torch.tanh(h_preact)\n",
    "    logits = b2 + h @ W2\n",
    "    return F.cross_entropy(logits, Y_t)\n",
    "\n",
    "epochs: Final[int] = 200000\n",
    "loss = torch.tensor(1000.0)\n",
    "mini_batch_size: Final[int] = 128\n",
    "\n",
    "lossi: list[float] = []\n",
    "stepi = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.8050\n"
     ]
    }
   ],
   "source": [
    "# Mini-batch\n",
    "ix = torch.randint(0, X_train.shape[0], (mini_batch_size,))\n",
    "X_t_mini = X_train[ix]\n",
    "Y_t_mini = Y_train[ix]\n",
    "\n",
    "# Forward pass:\n",
    "\n",
    "# Get the embedding for the mini-batch.\n",
    "emb = C[X_t_mini]\n",
    "embcat = emb.view(-1, block_size * embed_dim)\n",
    "\n",
    "# Layer 1 starts.\n",
    "h_pre_bn = embcat @ W1 + b1\n",
    "\n",
    "# Batch normalization\n",
    "# dim=0 means that the rows are the ones to be eliminated:\n",
    "# adding all batch elements together.\n",
    "bn_mean_i = 1 / mini_batch_size * h_pre_bn.sum(dim=0, keepdim=True)\n",
    "bn_diff = h_pre_bn - bn_mean_i\n",
    "bn_diff_sq = bn_diff * bn_diff\n",
    "bn_var_i = 1 / (mini_batch_size - 1) * bn_diff_sq.sum(dim=0, keepdim=True)\n",
    "bn_std_i = (bn_var_i + 1e-5)**(0.5)\n",
    "bn_std_i_inverse = (bn_var_i + 1e-5)**(-0.5)\n",
    "bn_raw = bn_diff * bn_std_i_inverse\n",
    "h_pre_act = bn_raw * bn_gain + bn_bias\n",
    "\n",
    "# Nonlinear activation\n",
    "h = torch.tanh(h_pre_act)\n",
    "# Layer 1 ends.\n",
    "\n",
    "# Layer 2 starts.\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "# Calculate cross entropy loss: loss = F.cross_entropy(logits, Y_t_mini)\n",
    "logit_maxes = logits.max(dim=1, keepdim=True).values\n",
    "normalized_logits = logits - logit_maxes # for numerical stability\n",
    "counts = normalized_logits.exp()\n",
    "counts_sum = counts.sum(dim=1, keepdim=True)\n",
    "counts_sum_inverse = counts_sum**(-1)\n",
    "probs = counts * counts_sum_inverse\n",
    "log_probs = probs.log()\n",
    "loss = -log_probs[torch.arange(mini_batch_size), Y_t_mini].mean()\n",
    "# Layer 2 ends.\n",
    "\n",
    "with torch.no_grad():\n",
    "    bn_mean_running = 0.999 * bn_mean_running + 0.001 * bn_mean_i\n",
    "    bn_std_running = 0.999 * bn_std_running + 0.001 * bn_std_i\n",
    "\n",
    "# Backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "# Retain the gradient for all the intermediate variables\n",
    "for t in [\n",
    "    log_probs, probs, counts_sum_inverse, counts_sum, counts,\n",
    "    normalized_logits, logit_maxes, logits, h, h_pre_act, bn_raw,\n",
    "    bn_std_i_inverse, bn_std_i, bn_var_i, bn_diff_sq, bn_diff, bn_mean_i,\n",
    "    h_pre_bn, embcat, emb\n",
    "]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "print(f\"loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 300]), torch.Size([1, 300]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_raw.shape, bn_gain.shape, bn_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 27]), torch.Size([128, 27]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs.shape, probs.shape\n",
    "# loss = -(a + b + c) / 3\n",
    "# dloss/da = -1 / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 27]), torch.Size([128, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum_inverse.shape\n",
    "# c = a * b, but with tensors:\n",
    "# a[3x3] * b[3x1] ->\n",
    "# a11 * b11 + a12 * b21 + a13 * b31\n",
    "# a12 * b12 + a22 * b22 + a32 * b32\n",
    "# a13 * b13 + a23 * b23 + a33 * b33\n",
    "# c[3x3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 27]), torch.Size([128, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum.shape\n",
    "# a11 a12 a13 -> b1 (=sum(a11, a12, a13))\n",
    "# a21 a22 a23 -> b2 (=sum(a21, a22, a23))\n",
    "# a31 a32 a33 -> b3 (=sum(a31, a32, a33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 27])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 300]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_gain.shape, bn_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 300]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_diff.shape, bn_std_i_inverse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 300]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_std_i_inverse.shape, bn_var_i.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 300]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bn_var_i = 1 / (mini_batch_size - 1) * bn_diff_sq.sum(dim=0, keepdim=True)\n",
    "\n",
    "bn_diff_sq.shape, bn_var_i.shape\n",
    "\n",
    "# a11 a12 a13\n",
    "# a21 a22 a23\n",
    "# ->\n",
    "# b1  b2  b3\n",
    "# b1 = 1 / (mini_batch_size - 1) * (a11 + a21)\n",
    "# b2 = 1 / (mini_batch_size - 1) * (a12 + a22)\n",
    "# b3 = 1 / (mini_batch_size - 1) * (a13 + a23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 300]), torch.Size([128, 300]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_diff.shape, h_pre_bn.shape, bn_mean_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 300]), torch.Size([128, 300]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_mean_i.shape, h_pre_bn.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 90]), torch.Size([128, 3, 30]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embcat = emb.view(-1, block_size * embed_dim)\n",
    "embcat.shape, emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 30]) torch.Size([27, 30]) torch.Size([128, 3])\n",
      "tensor([[ 1, 19, 19],\n",
      "        [ 9, 14, 20],\n",
      "        [ 7, 18,  1],\n",
      "        [ 0, 20,  1],\n",
      "        [ 1, 18, 12]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# emb = C[X_t_mini]\n",
    "print(emb.shape, C.shape, X_t_mini.shape)\n",
    "print(X_t_mini[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check:\n",
      "log_probs          | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "probs              | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "counts_sum_inverse | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "counts_sum         | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "counts             | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "normalized_logits  | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "logit_maxes        | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "logits             | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "b2                 | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "W2                 | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "h                  | shape equal: True | exact equal: True  | approximate equal: True  | max_diff: 0.0\n",
      "h_pre_act          | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 2.3283064365386963e-10\n",
      "bn_raw             | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 2.3283064365386963e-10\n",
      "bn_gain            | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 1.862645149230957e-09\n",
      "bn_bias            | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 3.725290298461914e-09\n",
      "bn_std_i_inverse   | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 3.725290298461914e-09\n",
      "bn_var_i           | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 4.656612873077393e-10\n",
      "bn_diff_sq         | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 3.637978807091713e-12\n",
      "bn_diff            | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 2.3283064365386963e-10\n",
      "bn_mean_i          | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 3.725290298461914e-09\n",
      "h_pre_bn           | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 2.3283064365386963e-10\n",
      "embcat             | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 6.984919309616089e-10\n",
      "W1                 | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 9.313225746154785e-09\n",
      "b1                 | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 4.6566128730773926e-09\n",
      "emb                | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 6.984919309616089e-10\n",
      "C                  | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 8.381903171539307e-09\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all of the variables\n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "d_log_probs = torch.zeros_like(log_probs)\n",
    "d_log_probs[torch.arange(mini_batch_size), Y_t_mini] = -1 / mini_batch_size\n",
    "\n",
    "d_probs = (1.0 / probs) * d_log_probs\n",
    "\n",
    "d_counts_sum_inverse = (counts * d_probs).sum(dim=1, keepdim=True)\n",
    "\n",
    "# d_counts part1\n",
    "d_counts = counts_sum_inverse * d_probs\n",
    "\n",
    "d_counts_sum = (-1) * (counts_sum**(-2)) * d_counts_sum_inverse\n",
    "\n",
    "# d_counts part2\n",
    "# This can also be simplified to: d_counts += d_counts_sum\n",
    "# Because `d_counts`'s shape is already the same as `counts`.\n",
    "# We will do broadcasting. `torch.ones_like(counts)` is unnecessary.\n",
    "d_counts += torch.ones_like(counts) * d_counts_sum\n",
    "\n",
    "# `normalized_logits.exp()` is already `counts`.\n",
    "d_normalized_logits = counts * d_counts\n",
    "\n",
    "# d_logits part1\n",
    "d_logits = d_normalized_logits.clone()\n",
    "\n",
    "d_logit_maxes = -d_logits.sum(dim=1, keepdim=True)\n",
    "\n",
    "# d_logits part2\n",
    "d_logits += F.one_hot(logits.argmax(dim=1), num_classes=logits.shape[1]) * d_logit_maxes\n",
    "\n",
    "d_b2 = d_logits.sum(dim=0)\n",
    "d_W2 = h.T @ d_logits\n",
    "d_h = d_logits @ W2.T\n",
    "\n",
    "d_h_pre_act = (1.0 - h**2) * d_h\n",
    "\n",
    "d_bn_raw = bn_gain * d_h_pre_act # Broadcast happens here.\n",
    "d_bn_gain = (bn_raw * d_h_pre_act).sum(dim=0, keepdim=True)\n",
    "d_bn_bias = d_h_pre_act.sum(dim=0, keepdim=True)\n",
    "\n",
    "d_bn_diff = bn_std_i_inverse * d_bn_raw # Broadcast happens here.\n",
    "d_bn_std_i_inverse = (bn_diff * d_bn_raw).sum(dim=0, keepdim=True)\n",
    "\n",
    "d_bn_var_i = (-(0.5) * (bn_var_i + 1e-5)**(-1.5)) * d_bn_std_i_inverse\n",
    "\n",
    "d_bn_diff_sq = (1.0 / (mini_batch_size - 1)) * torch.ones_like(bn_diff_sq) * d_bn_var_i\n",
    "\n",
    "d_bn_diff += 2.0 * bn_diff * d_bn_diff_sq\n",
    "\n",
    "d_bn_mean_i = (-1.0) * d_bn_diff.sum(dim=0, keepdim=True)\n",
    "d_h_pre_bn = d_bn_diff.clone()\n",
    "\n",
    "d_h_pre_bn += (1.0 / mini_batch_size) * torch.ones_like(h_pre_bn) * d_bn_mean_i\n",
    "\n",
    "d_embcat = d_h_pre_bn @ W1.T\n",
    "d_W1 = embcat.T @ d_h_pre_bn\n",
    "d_b1 = d_h_pre_bn.sum(dim=0)\n",
    "\n",
    "d_emb = d_embcat.view(emb.shape)\n",
    "\n",
    "d_C = torch.zeros_like(C)\n",
    "for i in range(X_t_mini.shape[0]):\n",
    "    for j in range(X_t_mini.shape[1]):\n",
    "        k = X_t_mini[i, j]\n",
    "        d_C[k] += d_emb[i, j]\n",
    "\n",
    "# and checking the gradients with cmp_grad.\n",
    "print(\"Gradient check:\")\n",
    "cmp_grad(\"log_probs\", d_log_probs, log_probs)\n",
    "cmp_grad(\"probs\", d_probs, probs)\n",
    "cmp_grad(\"counts_sum_inverse\", d_counts_sum_inverse, counts_sum_inverse)\n",
    "cmp_grad(\"counts_sum\", d_counts_sum, counts_sum)\n",
    "cmp_grad(\"counts\", d_counts, counts)\n",
    "cmp_grad(\"normalized_logits\", d_normalized_logits, normalized_logits)\n",
    "cmp_grad(\"logit_maxes\", d_logit_maxes, logit_maxes)\n",
    "cmp_grad(\"logits\", d_logits, logits)\n",
    "cmp_grad(\"b2\", d_b2, b2)\n",
    "cmp_grad(\"W2\", d_W2, W2)\n",
    "cmp_grad(\"h\", d_h, h)\n",
    "cmp_grad(\"h_pre_act\", d_h_pre_act, h_pre_act)\n",
    "cmp_grad(\"bn_raw\", d_bn_raw, bn_raw)\n",
    "cmp_grad(\"bn_gain\", d_bn_gain, bn_gain)\n",
    "cmp_grad(\"bn_bias\", d_bn_bias, bn_bias)\n",
    "cmp_grad(\"bn_std_i_inverse\", d_bn_std_i_inverse, bn_std_i_inverse)\n",
    "cmp_grad(\"bn_var_i\", d_bn_var_i, bn_var_i)\n",
    "cmp_grad(\"bn_diff_sq\", d_bn_diff_sq, bn_diff_sq)\n",
    "cmp_grad(\"bn_diff\", d_bn_diff, bn_diff)\n",
    "cmp_grad(\"bn_mean_i\", d_bn_mean_i, bn_mean_i)\n",
    "cmp_grad(\"h_pre_bn\", d_h_pre_bn, h_pre_bn)\n",
    "cmp_grad(\"embcat\", d_embcat, embcat)\n",
    "cmp_grad(\"W1\", d_W1, W1)\n",
    "cmp_grad(\"b1\", d_b1, b1)\n",
    "cmp_grad(\"emb\", d_emb, emb)\n",
    "cmp_grad(\"C\", d_C, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 27]),\n",
       " torch.Size([128, 27]),\n",
       " torch.Size([128, 1]),\n",
       " torch.Size([128, 27]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_normalized_logits.shape, logits.shape, logit_maxes.shape, log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_logits: torch.Size([128, 27])\n",
      "h: torch.Size([128, 300])\n",
      "W2: torch.Size([300, 27])\n",
      "b2: torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "print(f\"d_logits: {d_logits.shape}\\nh: {h.shape}\\nW2: {W2.shape}\\nb2: {b2.shape}\")\n",
    "# Given logits = h @ W2 + b2\n",
    "# We know that:\n",
    "# d_h ([128, 300]) must be calculated with logits([128, 27]) and W2([300, 27])\n",
    "# So we need to get [128, 300] from [128, 27] and [300, 27]\n",
    "# which is [128, 27] @ [27, 300]\n",
    "# which is [128, 27] @ [300, 27].T\n",
    "# so\n",
    "# d_h = d_logits @ W2.T\n",
    "\n",
    "# We know that:\n",
    "# d_W2 ([300, 27]) must be calculated with h([128, 300]) and d_logits([128, 27])\n",
    "# So we need to get [300, 27] from [128, 300] and [128, 27]\n",
    "# which is [128, 300].T @ [128, 27]\n",
    "# so\n",
    "# d_W2 = h.T @ d_logits\n",
    "\n",
    "# We know that:\n",
    "# d_b2 ([27]) must be calculated with d_logits([128, 27])\n",
    "# So we need to get [27] from [128, 27]\n",
    "# which is [128, 27].sum(dim=0)\n",
    "# so\n",
    "# d_b2 = d_logits.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Derivative Calculation](assets/matrix_derivative.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 300])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_h_pre_act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 300]),\n",
       " torch.Size([128, 90]),\n",
       " torch.Size([90, 300]),\n",
       " torch.Size([300]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_h_pre_bn.shape, embcat.shape, W1.shape, b1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 300]), torch.Size([128, 300]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_bn_diff.shape, h_pre_bn.shape, bn_mean_i.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 300]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_bn_mean_i.shape, bn_mean_i.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.8049497604370117, diff: -4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(dim=1, keepdim=True).values\n",
    "# normalized_logits = logits - logit_maxes # for numerical stability\n",
    "# counts = normalized_logits.exp()\n",
    "# counts_sum = counts.sum(dim=1, keepdim=True)\n",
    "# counts_sum_inverse = counts_sum**(-1)\n",
    "# probs = counts * counts_sum_inverse\n",
    "# log_probs = probs.log()\n",
    "# loss = -log_probs[torch.arange(mini_batch_size), Y_t_mini].mean()\n",
    "\n",
    "# now\n",
    "loss_fast = F.cross_entropy(logits, Y_t_mini)\n",
    "print(f\"loss: {loss_fast.item()}, diff: {(loss_fast - loss).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 27])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits             | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 1.862645149230957e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# softmax over 27 characters.\n",
    "prob_logit = F.softmax(logits, dim=1)\n",
    "\n",
    "# when the predicted label match the target label: Pi - 1\n",
    "prob_logit[range(mini_batch_size), Y_t_mini] -= 1\n",
    "\n",
    "# when the predicted label doesn't match the target label: Pi\n",
    "# unchanged\n",
    "\n",
    "d_logits = prob_logit / mini_batch_size\n",
    "\n",
    "cmp_grad(\"logits\", d_logits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cross Entry Derivative](assets/cross_entrypy_derivative.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: 7.152557373046875e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bn_mean_i = 1 / mini_batch_size * h_pre_bn.sum(dim=0, keepdim=True)\n",
    "# bn_diff = h_pre_bn - bn_mean_i\n",
    "# bn_diff_sq = bn_diff * bn_diff\n",
    "# bn_var_i = 1 / (mini_batch_size - 1) * bn_diff_sq.sum(dim=0, keepdim=True)\n",
    "# bn_std_i = (bn_var_i + 1e-5)**(0.5)\n",
    "# bn_std_i_inverse = (bn_var_i + 1e-5)**(-0.5)\n",
    "# bn_raw = bn_diff * bn_std_i_inverse\n",
    "# h_pre_act = bn_raw * bn_gain + bn_bias\n",
    "\n",
    "# now:\n",
    "h_pre_act_fast = ((bn_gain * (h_pre_bn - h_pre_bn.mean(dim=0, keepdim=True))) /\n",
    "                torch.sqrt(h_pre_bn.var(dim=0, keepdim=True, unbiased=True) + 1e-5) +\n",
    "                bn_bias)\n",
    "print(f\"max diff: {(h_pre_act_fast - h_pre_act).abs().max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 300]),\n",
       " torch.Size([1, 300]),\n",
       " torch.Size([128, 300]),\n",
       " torch.Size([128, 300]),\n",
       " torch.Size([128, 300]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_gain.shape, bn_std_i_inverse.shape, d_h_pre_act.shape, bn_raw.shape, h_pre_bn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_pre_bn           | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 4.656612873077393e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# Before:\n",
    "# d_bn_raw = bn_gain * d_h_pre_act # Broadcast happens here.\n",
    "# d_bn_gain = (bn_raw * d_h_pre_act).sum(dim=0, keepdim=True)\n",
    "# d_bn_bias = d_h_pre_act.sum(dim=0, keepdim=True)\n",
    "\n",
    "# d_bn_diff = bn_std_i_inverse * d_bn_raw # Broadcast happens here.\n",
    "# d_bn_std_i_inverse = (bn_diff * d_bn_raw).sum(dim=0, keepdim=True)\n",
    "\n",
    "# d_bn_var_i = (-(0.5) * (bn_var_i + 1e-5)**(-1.5)) * d_bn_std_i_inverse\n",
    "\n",
    "# d_bn_diff_sq = (1.0 / (mini_batch_size - 1)) * torch.ones_like(bn_diff_sq) * d_bn_var_i\n",
    "\n",
    "# d_bn_diff += 2.0 * bn_diff * d_bn_diff_sq\n",
    "\n",
    "# d_bn_mean_i = (-1.0) * d_bn_diff.sum(dim=0, keepdim=True)\n",
    "# d_h_pre_bn = d_bn_diff.clone()\n",
    "\n",
    "# d_h_pre_bn += (1.0 / mini_batch_size) * torch.ones_like(h_pre_bn) * d_bn_mean_i\n",
    "\n",
    "d_h_pre_bn = (\n",
    "    bn_gain * bn_std_i_inverse / mini_batch_size *\n",
    "    (\n",
    "        (mini_batch_size * d_h_pre_act) - d_h_pre_act.sum(dim=0, keepdim=True) -\n",
    "        (mini_batch_size / (mini_batch_size - 1) * bn_raw * ((d_h_pre_act * bn_raw).sum(dim=0, keepdim=True)))\n",
    "    )\n",
    ")\n",
    "\n",
    "cmp_grad(\"h_pre_bn\", d_h_pre_bn, h_pre_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/batch_norm_derivative_1.jpg)\n",
    "![](assets/batch_norm_derivative_2.jpg)\n",
    "![](assets/batch_norm_derivative_3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 36837\n",
      "Epoch: 0 / 200000, Loss: 4.106904983520508\n",
      "Epoch: 1000 / 200000, Loss: 2.138272285461426\n",
      "Epoch: 2000 / 200000, Loss: 2.1745712757110596\n",
      "Epoch: 3000 / 200000, Loss: 2.3317978382110596\n",
      "Epoch: 4000 / 200000, Loss: 2.380472183227539\n",
      "Epoch: 5000 / 200000, Loss: 2.036611795425415\n",
      "Epoch: 6000 / 200000, Loss: 2.2435784339904785\n",
      "Epoch: 7000 / 200000, Loss: 2.3522582054138184\n",
      "Epoch: 8000 / 200000, Loss: 1.9891908168792725\n",
      "Epoch: 9000 / 200000, Loss: 2.195371150970459\n",
      "Epoch: 10000 / 200000, Loss: 2.1620936393737793\n",
      "Epoch: 11000 / 200000, Loss: 2.119335412979126\n",
      "Epoch: 12000 / 200000, Loss: 2.2042179107666016\n",
      "Epoch: 13000 / 200000, Loss: 2.084089756011963\n",
      "Epoch: 14000 / 200000, Loss: 2.0471060276031494\n",
      "Epoch: 15000 / 200000, Loss: 2.0057532787323\n",
      "Epoch: 16000 / 200000, Loss: 2.225254774093628\n",
      "Epoch: 17000 / 200000, Loss: 1.8110008239746094\n",
      "Epoch: 18000 / 200000, Loss: 2.106604814529419\n",
      "Epoch: 19000 / 200000, Loss: 2.1003613471984863\n",
      "Epoch: 20000 / 200000, Loss: 2.1749444007873535\n",
      "Epoch: 21000 / 200000, Loss: 2.085662364959717\n",
      "Epoch: 22000 / 200000, Loss: 2.165099620819092\n",
      "Epoch: 23000 / 200000, Loss: 2.017554521560669\n",
      "Epoch: 24000 / 200000, Loss: 2.178191661834717\n",
      "Epoch: 25000 / 200000, Loss: 2.054765462875366\n",
      "Epoch: 26000 / 200000, Loss: 2.1027252674102783\n",
      "Epoch: 27000 / 200000, Loss: 2.0701944828033447\n",
      "Epoch: 28000 / 200000, Loss: 2.0530283451080322\n",
      "Epoch: 29000 / 200000, Loss: 2.136361598968506\n",
      "Epoch: 30000 / 200000, Loss: 1.942742109298706\n",
      "Epoch: 31000 / 200000, Loss: 2.319347858428955\n",
      "Epoch: 32000 / 200000, Loss: 2.106630802154541\n",
      "Epoch: 33000 / 200000, Loss: 2.0114998817443848\n",
      "Epoch: 34000 / 200000, Loss: 1.9311652183532715\n",
      "Epoch: 35000 / 200000, Loss: 1.7782816886901855\n",
      "Epoch: 36000 / 200000, Loss: 2.146813154220581\n",
      "Epoch: 37000 / 200000, Loss: 1.941052794456482\n",
      "Epoch: 38000 / 200000, Loss: 2.0632517337799072\n",
      "Epoch: 39000 / 200000, Loss: 2.077777862548828\n",
      "Epoch: 40000 / 200000, Loss: 1.9421542882919312\n",
      "Epoch: 41000 / 200000, Loss: 1.807384967803955\n",
      "Epoch: 42000 / 200000, Loss: 2.076383113861084\n",
      "Epoch: 43000 / 200000, Loss: 2.1277809143066406\n",
      "Epoch: 44000 / 200000, Loss: 2.153531074523926\n",
      "Epoch: 45000 / 200000, Loss: 1.9547102451324463\n",
      "Epoch: 46000 / 200000, Loss: 2.0490474700927734\n",
      "Epoch: 47000 / 200000, Loss: 1.9667836427688599\n",
      "Epoch: 48000 / 200000, Loss: 2.0318362712860107\n",
      "Epoch: 49000 / 200000, Loss: 2.008068323135376\n",
      "Epoch: 50000 / 200000, Loss: 1.94374680519104\n",
      "Epoch: 51000 / 200000, Loss: 1.9057397842407227\n",
      "Epoch: 52000 / 200000, Loss: 2.0044801235198975\n",
      "Epoch: 53000 / 200000, Loss: 1.9964196681976318\n",
      "Epoch: 54000 / 200000, Loss: 1.959145188331604\n",
      "Epoch: 55000 / 200000, Loss: 2.0145785808563232\n",
      "Epoch: 56000 / 200000, Loss: 1.928016185760498\n",
      "Epoch: 57000 / 200000, Loss: 1.9366344213485718\n",
      "Epoch: 58000 / 200000, Loss: 1.9768773317337036\n",
      "Epoch: 59000 / 200000, Loss: 2.0267438888549805\n",
      "Epoch: 60000 / 200000, Loss: 2.012028932571411\n",
      "Epoch: 61000 / 200000, Loss: 1.8771159648895264\n",
      "Epoch: 62000 / 200000, Loss: 1.9134429693222046\n",
      "Epoch: 63000 / 200000, Loss: 1.9293752908706665\n",
      "Epoch: 64000 / 200000, Loss: 2.014953136444092\n",
      "Epoch: 65000 / 200000, Loss: 2.1259868144989014\n",
      "Epoch: 66000 / 200000, Loss: 1.804625391960144\n",
      "Epoch: 67000 / 200000, Loss: 1.8440532684326172\n",
      "Epoch: 68000 / 200000, Loss: 1.9934577941894531\n",
      "Epoch: 69000 / 200000, Loss: 1.9178006649017334\n",
      "Epoch: 70000 / 200000, Loss: 2.1345813274383545\n",
      "Epoch: 71000 / 200000, Loss: 1.8999067544937134\n",
      "Epoch: 72000 / 200000, Loss: 2.1354129314422607\n",
      "Epoch: 73000 / 200000, Loss: 1.9864819049835205\n",
      "Epoch: 74000 / 200000, Loss: 2.119917154312134\n",
      "Epoch: 75000 / 200000, Loss: 2.2346723079681396\n",
      "Epoch: 76000 / 200000, Loss: 2.058151960372925\n",
      "Epoch: 77000 / 200000, Loss: 2.0549302101135254\n",
      "Epoch: 78000 / 200000, Loss: 2.154261827468872\n",
      "Epoch: 79000 / 200000, Loss: 2.1366915702819824\n",
      "Epoch: 80000 / 200000, Loss: 1.9419142007827759\n",
      "Epoch: 81000 / 200000, Loss: 1.8733841180801392\n",
      "Epoch: 82000 / 200000, Loss: 1.9923204183578491\n",
      "Epoch: 83000 / 200000, Loss: 1.791207194328308\n",
      "Epoch: 84000 / 200000, Loss: 1.9696625471115112\n",
      "Epoch: 85000 / 200000, Loss: 2.234699010848999\n",
      "Epoch: 86000 / 200000, Loss: 2.015516996383667\n",
      "Epoch: 87000 / 200000, Loss: 1.8965604305267334\n",
      "Epoch: 88000 / 200000, Loss: 2.318376064300537\n",
      "Epoch: 89000 / 200000, Loss: 1.9529039859771729\n",
      "Epoch: 90000 / 200000, Loss: 1.9738233089447021\n",
      "Epoch: 91000 / 200000, Loss: 1.9745690822601318\n",
      "Epoch: 92000 / 200000, Loss: 2.0123097896575928\n",
      "Epoch: 93000 / 200000, Loss: 1.9956316947937012\n",
      "Epoch: 94000 / 200000, Loss: 2.0879883766174316\n",
      "Epoch: 95000 / 200000, Loss: 2.0060160160064697\n",
      "Epoch: 96000 / 200000, Loss: 2.0460851192474365\n",
      "Epoch: 97000 / 200000, Loss: 1.979703426361084\n",
      "Epoch: 98000 / 200000, Loss: 2.0579779148101807\n",
      "Epoch: 99000 / 200000, Loss: 1.9915002584457397\n",
      "Epoch: 100000 / 200000, Loss: 1.9818625450134277\n",
      "Epoch: 101000 / 200000, Loss: 1.9484000205993652\n",
      "Epoch: 102000 / 200000, Loss: 2.083057165145874\n",
      "Epoch: 103000 / 200000, Loss: 1.9715384244918823\n",
      "Epoch: 104000 / 200000, Loss: 2.0901260375976562\n",
      "Epoch: 105000 / 200000, Loss: 1.967293620109558\n",
      "Epoch: 106000 / 200000, Loss: 1.9806097745895386\n",
      "Epoch: 107000 / 200000, Loss: 2.0569112300872803\n",
      "Epoch: 108000 / 200000, Loss: 2.001068115234375\n",
      "Epoch: 109000 / 200000, Loss: 2.0338268280029297\n",
      "Epoch: 110000 / 200000, Loss: 2.1698179244995117\n",
      "Epoch: 111000 / 200000, Loss: 2.203782796859741\n",
      "Epoch: 112000 / 200000, Loss: 1.8805550336837769\n",
      "Epoch: 113000 / 200000, Loss: 1.923356294631958\n",
      "Epoch: 114000 / 200000, Loss: 2.1546969413757324\n",
      "Epoch: 115000 / 200000, Loss: 1.8726545572280884\n",
      "Epoch: 116000 / 200000, Loss: 1.9788821935653687\n",
      "Epoch: 117000 / 200000, Loss: 1.9622013568878174\n",
      "Epoch: 118000 / 200000, Loss: 2.0305051803588867\n",
      "Epoch: 119000 / 200000, Loss: 2.03273344039917\n",
      "Epoch: 120000 / 200000, Loss: 2.0108187198638916\n",
      "Epoch: 121000 / 200000, Loss: 1.8743908405303955\n",
      "Epoch: 122000 / 200000, Loss: 1.8909751176834106\n",
      "Epoch: 123000 / 200000, Loss: 1.860060453414917\n",
      "Epoch: 124000 / 200000, Loss: 2.052785873413086\n",
      "Epoch: 125000 / 200000, Loss: 2.104322671890259\n",
      "Epoch: 126000 / 200000, Loss: 2.0359232425689697\n",
      "Epoch: 127000 / 200000, Loss: 2.1133649349212646\n",
      "Epoch: 128000 / 200000, Loss: 1.9666801691055298\n",
      "Epoch: 129000 / 200000, Loss: 1.8451838493347168\n",
      "Epoch: 130000 / 200000, Loss: 1.960326075553894\n",
      "Epoch: 131000 / 200000, Loss: 1.9223395586013794\n",
      "Epoch: 132000 / 200000, Loss: 1.8878567218780518\n",
      "Epoch: 133000 / 200000, Loss: 1.9098014831542969\n",
      "Epoch: 134000 / 200000, Loss: 2.0739355087280273\n",
      "Epoch: 135000 / 200000, Loss: 2.0500824451446533\n",
      "Epoch: 136000 / 200000, Loss: 2.009234666824341\n",
      "Epoch: 137000 / 200000, Loss: 2.1219725608825684\n",
      "Epoch: 138000 / 200000, Loss: 2.007319688796997\n",
      "Epoch: 139000 / 200000, Loss: 1.9332902431488037\n",
      "Epoch: 140000 / 200000, Loss: 2.2141401767730713\n",
      "Epoch: 141000 / 200000, Loss: 2.181128978729248\n",
      "Epoch: 142000 / 200000, Loss: 2.049124240875244\n",
      "Epoch: 143000 / 200000, Loss: 1.9139554500579834\n",
      "Epoch: 144000 / 200000, Loss: 1.9320719242095947\n",
      "Epoch: 145000 / 200000, Loss: 1.9774236679077148\n",
      "Epoch: 146000 / 200000, Loss: 1.9216148853302002\n",
      "Epoch: 147000 / 200000, Loss: 1.908438801765442\n",
      "Epoch: 148000 / 200000, Loss: 2.0587973594665527\n",
      "Epoch: 149000 / 200000, Loss: 2.028285503387451\n",
      "Epoch: 150000 / 200000, Loss: 2.1063008308410645\n",
      "Epoch: 151000 / 200000, Loss: 2.1369500160217285\n",
      "Epoch: 152000 / 200000, Loss: 1.9186338186264038\n",
      "Epoch: 153000 / 200000, Loss: 1.9379749298095703\n",
      "Epoch: 154000 / 200000, Loss: 2.151502847671509\n",
      "Epoch: 155000 / 200000, Loss: 2.3372907638549805\n",
      "Epoch: 156000 / 200000, Loss: 1.9589250087738037\n",
      "Epoch: 157000 / 200000, Loss: 1.992610216140747\n",
      "Epoch: 158000 / 200000, Loss: 1.8143723011016846\n",
      "Epoch: 159000 / 200000, Loss: 1.9217777252197266\n",
      "Epoch: 160000 / 200000, Loss: 1.9855166673660278\n",
      "Epoch: 161000 / 200000, Loss: 2.031088352203369\n",
      "Epoch: 162000 / 200000, Loss: 2.1593196392059326\n",
      "Epoch: 163000 / 200000, Loss: 1.99331796169281\n",
      "Epoch: 164000 / 200000, Loss: 2.07643461227417\n",
      "Epoch: 165000 / 200000, Loss: 2.12296462059021\n",
      "Epoch: 166000 / 200000, Loss: 1.8986518383026123\n",
      "Epoch: 167000 / 200000, Loss: 1.9582271575927734\n",
      "Epoch: 168000 / 200000, Loss: 1.889273762702942\n",
      "Epoch: 169000 / 200000, Loss: 1.7325401306152344\n",
      "Epoch: 170000 / 200000, Loss: 1.9971691370010376\n",
      "Epoch: 171000 / 200000, Loss: 2.0494747161865234\n",
      "Epoch: 172000 / 200000, Loss: 1.8812408447265625\n",
      "Epoch: 173000 / 200000, Loss: 2.1222290992736816\n",
      "Epoch: 174000 / 200000, Loss: 1.9971604347229004\n",
      "Epoch: 175000 / 200000, Loss: 1.986293911933899\n",
      "Epoch: 176000 / 200000, Loss: 1.9820055961608887\n",
      "Epoch: 177000 / 200000, Loss: 2.0753698348999023\n",
      "Epoch: 178000 / 200000, Loss: 1.9142059087753296\n",
      "Epoch: 179000 / 200000, Loss: 1.9173234701156616\n",
      "Epoch: 180000 / 200000, Loss: 2.0035481452941895\n",
      "Epoch: 181000 / 200000, Loss: 1.951196551322937\n",
      "Epoch: 182000 / 200000, Loss: 1.9226151704788208\n",
      "Epoch: 183000 / 200000, Loss: 1.9462394714355469\n",
      "Epoch: 184000 / 200000, Loss: 2.1642558574676514\n",
      "Epoch: 185000 / 200000, Loss: 1.8529983758926392\n",
      "Epoch: 186000 / 200000, Loss: 2.1077492237091064\n",
      "Epoch: 187000 / 200000, Loss: 1.9956976175308228\n",
      "Epoch: 188000 / 200000, Loss: 1.9349365234375\n",
      "Epoch: 189000 / 200000, Loss: 2.0710055828094482\n",
      "Epoch: 190000 / 200000, Loss: 1.9450242519378662\n",
      "Epoch: 191000 / 200000, Loss: 2.021679162979126\n",
      "Epoch: 192000 / 200000, Loss: 1.7476028203964233\n",
      "Epoch: 193000 / 200000, Loss: 2.2263522148132324\n",
      "Epoch: 194000 / 200000, Loss: 1.9681003093719482\n",
      "Epoch: 195000 / 200000, Loss: 2.005915641784668\n",
      "Epoch: 196000 / 200000, Loss: 1.9878005981445312\n",
      "Epoch: 197000 / 200000, Loss: 1.9619356393814087\n",
      "Epoch: 198000 / 200000, Loss: 1.8597304821014404\n",
      "Epoch: 199000 / 200000, Loss: 2.0753567218780518\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together\n",
    "# Train the MLP neural net with our own backward pass\n",
    "\n",
    "num_neurons: Final[int] = 300\n",
    "embed_dim: Final[int] = 30\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((char_cnt, embed_dim), generator=g).to(device)\n",
    "# Layer 1\n",
    "W1 = torch.randn((block_size * embed_dim, num_neurons), generator=g).to(device) * (5 / 3) / ((embed_dim * block_size) ** 0.5)\n",
    "b1 = torch.randn(num_neurons, generator=g).to(device) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((num_neurons, char_cnt), generator=g).to(device) * 0.1\n",
    "b2 = torch.randn(char_cnt, generator=g).to(device) * 0.1\n",
    "\n",
    "# Batch normalization parameters\n",
    "bn_gain = torch.randn((1, num_neurons), generator=g).to(device) * 0.1 + 1.0\n",
    "bn_bias = torch.randn((1, num_neurons), generator=g).to(device) * 0.1\n",
    "bn_mean_running = torch.zeros((1, num_neurons)).to(device)\n",
    "bn_std_running = torch.ones((1, num_neurons)).to(device)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(f\"parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_loss(X_t: torch.Tensor, Y_t: torch.Tensor) -> torch.Tensor:\n",
    "    emb = C[X_t]\n",
    "    embcat = emb.view(-1, block_size * embed_dim)\n",
    "    h_preact = embcat @ W1 + b1\n",
    "    # Use the running mean/std.\n",
    "    h_preact_norm = (h_preact - bn_mean_running) / bn_std_running\n",
    "    h_preact = h_preact_norm * bn_gain + bn_bias\n",
    "    h = torch.tanh(h_preact)\n",
    "    logits = b2 + h @ W2\n",
    "    return F.cross_entropy(logits, Y_t)\n",
    "\n",
    "epochs: Final[int] = 200000\n",
    "loss = torch.tensor(1000.0)\n",
    "mini_batch_size: Final[int] = 128\n",
    "\n",
    "lossi: list[float] = []\n",
    "stepi = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for epoch in range(epochs):\n",
    "        # Mini-batch\n",
    "        ix = torch.randint(0, X_train.shape[0], (mini_batch_size,))\n",
    "        X_t_mini = X_train[ix]\n",
    "        Y_t_mini = Y_train[ix]\n",
    "\n",
    "        # Forward pass:\n",
    "\n",
    "        # Get the embedding for the mini-batch.\n",
    "        emb = C[X_t_mini]\n",
    "        embcat = emb.view(-1, block_size * embed_dim)\n",
    "\n",
    "        # Layer 1 starts.\n",
    "        h_pre_bn = embcat @ W1 + b1\n",
    "\n",
    "        # Batch normalization\n",
    "        # dim=0 means that the rows are the ones to be eliminated:\n",
    "        # adding all batch elements together.\n",
    "        bn_mean_i = 1 / mini_batch_size * h_pre_bn.sum(dim=0, keepdim=True)\n",
    "        bn_diff = h_pre_bn - bn_mean_i\n",
    "        bn_diff_sq = bn_diff * bn_diff\n",
    "        bn_var_i = 1 / (mini_batch_size - 1) * bn_diff_sq.sum(dim=0, keepdim=True)\n",
    "        bn_std_i = (bn_var_i + 1e-5)**(0.5)\n",
    "        bn_std_i_inverse = (bn_var_i + 1e-5)**(-0.5)\n",
    "        bn_raw = bn_diff * bn_std_i_inverse\n",
    "        h_pre_act = bn_raw * bn_gain + bn_bias\n",
    "\n",
    "        # Nonlinear activation\n",
    "        h = torch.tanh(h_pre_act)\n",
    "        # Layer 1 ends.\n",
    "\n",
    "        # Layer 2 starts.\n",
    "        logits = h @ W2 + b2\n",
    "\n",
    "        # Calculate cross entropy loss: loss = F.cross_entropy(logits, Y_t_mini)\n",
    "        # logit_maxes = logits.max(dim=1, keepdim=True).values\n",
    "        # normalized_logits = logits - logit_maxes # for numerical stability\n",
    "        # counts = normalized_logits.exp()\n",
    "        # counts_sum = counts.sum(dim=1, keepdim=True)\n",
    "        # counts_sum_inverse = counts_sum**(-1)\n",
    "        # probs = counts * counts_sum_inverse\n",
    "        # log_probs = probs.log()\n",
    "        # loss = -log_probs[torch.arange(mini_batch_size), Y_t_mini].mean()\n",
    "        loss = F.cross_entropy(logits, Y_t_mini)\n",
    "        # Layer 2 ends.\n",
    "\n",
    "        if (epoch % 1000) == 0:\n",
    "            print(f\"Epoch: {epoch} / {epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bn_mean_running = 0.999 * bn_mean_running + 0.001 * bn_mean_i\n",
    "            bn_std_running = 0.999 * bn_std_running + 0.001 * bn_std_i\n",
    "\n",
    "        # Backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        # loss.backward()\n",
    "\n",
    "        # Manual backprop!\n",
    "        # parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "        # ----------------\n",
    "\n",
    "        # Cross entrypy\n",
    "        # softmax over 27 characters.\n",
    "        prob_logit = F.softmax(logits, dim=1)\n",
    "        # when the predicted label match the target label: Pi - 1\n",
    "        prob_logit[range(mini_batch_size), Y_t_mini] -= 1\n",
    "        # when the predicted label doesn't match the target label: Pi\n",
    "        # unchanged\n",
    "        d_logits = prob_logit / mini_batch_size\n",
    "\n",
    "        # Layer two\n",
    "        d_b2 = d_logits.sum(dim=0)\n",
    "        d_W2 = h.T @ d_logits\n",
    "        d_h = d_logits @ W2.T\n",
    "\n",
    "        # tanh\n",
    "        d_h_pre_act = (1.0 - h**2) * d_h\n",
    "\n",
    "        # Batch norm\n",
    "        d_bn_raw = bn_gain * d_h_pre_act # Broadcast happens here.\n",
    "        d_bn_gain = (bn_raw * d_h_pre_act).sum(dim=0, keepdim=True)\n",
    "        d_bn_bias = d_h_pre_act.sum(dim=0, keepdim=True)\n",
    "\n",
    "        d_h_pre_bn = (\n",
    "            bn_gain * bn_std_i_inverse / mini_batch_size *\n",
    "            (\n",
    "                (mini_batch_size * d_h_pre_act) - d_h_pre_act.sum(dim=0, keepdim=True) -\n",
    "                (mini_batch_size / (mini_batch_size - 1) * bn_raw * ((d_h_pre_act * bn_raw).sum(dim=0, keepdim=True)))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        d_embcat = d_h_pre_bn @ W1.T\n",
    "        d_W1 = embcat.T @ d_h_pre_bn\n",
    "        d_b1 = d_h_pre_bn.sum(dim=0)\n",
    "\n",
    "        d_emb = d_embcat.view(emb.shape)\n",
    "\n",
    "        d_C = torch.zeros_like(C)\n",
    "        for i in range(X_t_mini.shape[0]):\n",
    "            for j in range(X_t_mini.shape[1]):\n",
    "                k = X_t_mini[i, j]\n",
    "                d_C[k] += d_emb[i, j]\n",
    "\n",
    "        grads: Final[list[torch.Tensor]] = [d_C, d_W1, d_b1, d_W2, d_b2, d_bn_gain, d_bn_bias]\n",
    "        # ----------------\n",
    "\n",
    "        lr = (0.1 if epoch < 25000 else\n",
    "            0.05 if epoch < 100000 else\n",
    "            0.01 if epoch < 150000 else\n",
    "            0.005 if epoch < 180000 else\n",
    "            0.001)\n",
    "\n",
    "        # Update the parameters\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            # assert torch.allclose(grad, p.grad)\n",
    "            p.data += -lr * grad\n",
    "\n",
    "        stepi.append(epoch)\n",
    "        lossi.append(loss.log10().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 30)           | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 1.0244548320770264e-08\n",
      "(90, 300)          | shape equal: True | exact equal: False | approximate equal: False | max_diff: 1.6763806343078613e-08\n",
      "(300,)             | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 7.450580596923828e-09\n",
      "(300, 27)          | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 1.30385160446167e-08\n",
      "(27,)              | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 3.725290298461914e-09\n",
      "(1, 300)           | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 1.5133991837501526e-09\n",
      "(1, 300)           | shape equal: True | exact equal: False | approximate equal: True  | max_diff: 2.561137080192566e-09\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to check gradients.\n",
    "# for p, grad in zip(parameters, grads):\n",
    "#     cmp_grad(str(tuple(p.shape)), grad, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn_mean_oneshot: torch.Size([1, 300]), bn_var_oneshot: torch.Size([1, 300])\n",
      "bn_mean_running: torch.Size([1, 300]), bn_std_running: torch.Size([1, 300])\n",
      "bn_mean_running_diff: 0.009615212678909302\n",
      "bn_std_running_diff: 0.012670397758483887\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    emb = C[X_train]\n",
    "    embcat = emb.view(-1, block_size * embed_dim)\n",
    "    h_pre_act = embcat @ W1 + b1\n",
    "    # measure the mean/std over the entire training dataset\n",
    "    bn_mean_oneshot = h_pre_act.mean(dim=0, keepdim=True)\n",
    "    bn_var_oneshot = h_pre_act.var(dim=0, keepdim=True, unbiased=True)\n",
    "    bn_std_oneshot = (bn_var_oneshot + 1e-5)**(0.5)\n",
    "\n",
    "print(f\"bn_mean_oneshot: {bn_mean_oneshot.shape}, bn_var_oneshot: {bn_var_oneshot.shape}\")\n",
    "print(f\"bn_mean_running: {bn_mean_running.shape}, bn_std_running: {bn_std_running.shape}\")\n",
    "\n",
    "# Check if the running mean and std are close to the ones calculated\n",
    "# Use allclose and calculate max_diff.\n",
    "bn_mean_running_diff = (bn_mean_running - bn_mean_oneshot).abs().max()\n",
    "bn_std_running_diff = (bn_std_running - bn_std_oneshot).abs().max()\n",
    "print(f\"bn_mean_running_diff: {bn_mean_running_diff.item()}\")\n",
    "print(f\"bn_std_running_diff: {bn_std_running_diff.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set loss: 1.967294692993164\n",
      "Validation set loss: 2.0674850940704346\n",
      "Test set loss: 2.0659079551696777\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_loss(X_t: torch.Tensor, Y_t: torch.Tensor) -> torch.Tensor:\n",
    "    emb = C[X_t]\n",
    "    embcat = emb.view(-1, block_size * embed_dim)\n",
    "    h_preact = embcat @ W1 + b1\n",
    "    # Use the running mean/std.\n",
    "    h_preact_norm = (h_preact - bn_mean_oneshot) / bn_std_oneshot\n",
    "    h_preact = h_preact_norm * bn_gain + bn_bias\n",
    "    h = torch.tanh(h_preact)\n",
    "    logits = b2 + h @ W2\n",
    "    return F.cross_entropy(logits, Y_t)\n",
    "\n",
    "print(f\"Train set loss: {calculate_loss(X_train, Y_train).item()}\")\n",
    "print(f\"Validation set loss: {calculate_loss(X_val, Y_val).item()}\")\n",
    "print(f\"Test set loss: {calculate_loss(X_test, Y_test).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeen\n",
      "moca\n",
      "asia\n",
      "meed\n",
      "wakenley\n",
      "aly\n",
      "maryairgette\n",
      "orie\n",
      "sanney\n",
      "rahman\n",
      "finny\n",
      "lan\n",
      "kamset\n",
      "wohsinciel\n",
      "nataleslyn\n",
      "geochenrie\n",
      "natherson\n",
      "bluel\n",
      "dor\n",
      "amyalilah\n"
     ]
    }
   ],
   "source": [
    "# Calibrate the batch normalization at the end of the training.\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(num_samples: int) -> list[str]:\n",
    "    C_cpu = C.to('cpu')\n",
    "    W1_cpu = W1.to('cpu')\n",
    "    b1_cpu = b1.to('cpu')\n",
    "    W2_cpu = W2.to('cpu')\n",
    "    b2_cpu = b2.to('cpu')\n",
    "\n",
    "    bn_mean_running_cpu = bn_mean_oneshot.to('cpu')\n",
    "    bn_std_running_cpu = bn_std_oneshot.to('cpu')\n",
    "    bn_gain_cpu = bn_gain.to(\"cpu\")\n",
    "    bn_bias_cpu = bn_bias.to(\"cpu\")\n",
    "\n",
    "    names: list[str] = []\n",
    "    for _ in range(num_samples):\n",
    "        out: list[str] = []\n",
    "        context: list[int] = [0] * block_size\n",
    "        while True:\n",
    "            emb = C_cpu[torch.tensor(context)]\n",
    "            embcat = emb.view(1, -1)\n",
    "            h_preact = embcat @ W1_cpu + b1_cpu\n",
    "            h_preact_norm = (h_preact - bn_mean_running_cpu) / bn_std_running_cpu\n",
    "            h = torch.tanh(h_preact_norm * bn_gain_cpu + bn_bias_cpu)\n",
    "            logits = b2_cpu + h @ W2_cpu\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix: int = int(\n",
    "                torch.multinomial(probs, num_samples=1).item()\n",
    "            )\n",
    "            context = context[1:] + [ix]\n",
    "            out.append(itos[ix])\n",
    "            if ix == 0:\n",
    "                break\n",
    "        names.append(''.join(out))\n",
    "    return names\n",
    "\n",
    "names: list[str] = sample(20)\n",
    "for name in names:\n",
    "    print(name[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
