{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final, Tuple\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words: Final[list[str]] = open(\"names.txt\").read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set: Final[set[str]] = set(list(\"\".join(words)))\n",
    "char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 1, 'e': 2, 't': 3, 'f': 4, 'o': 5, 'r': 6, 'b': 7, 'y': 8, 'v': 9, 'i': 10, 'c': 11, 'p': 12, 'h': 13, 'u': 14, 'z': 15, 'l': 16, 'x': 17, 'j': 18, 's': 19, 'q': 20, 'w': 21, 'd': 22, 'm': 23, 'g': 24, 'a': 25, 'k': 26, '.': 0}\n",
      "{1: 'n', 2: 'e', 3: 't', 4: 'f', 5: 'o', 6: 'r', 7: 'b', 8: 'y', 9: 'v', 10: 'i', 11: 'c', 12: 'p', 13: 'h', 14: 'u', 15: 'z', 16: 'l', 17: 'x', 18: 'j', 19: 's', 20: 'q', 21: 'w', 22: 'd', 23: 'm', 24: 'g', 25: 'a', 26: 'k', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "stoi: Final[dict[str, int]] = {\n",
    "    ch: i + 1\n",
    "    for i, ch in enumerate(char_set)\n",
    "}\n",
    "stoi['.'] = 0\n",
    "itos: Final[dict[int, str]] = {i: ch for ch, i in stoi.items()}\n",
    "char_count: Final[int] = len(stoi)\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs: Final[list[Tuple[int, int]]] = []\n",
    "ys: Final[list[int]] = []\n",
    "for w in words[:1]:\n",
    "    chs: list[str] = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        xs.append((stoi[ch1], stoi[ch2]))\n",
    "        ys.append(stoi[ch3])\n",
    "xs_ts = torch.tensor(xs)\n",
    "ys_ts = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  2],\n",
      "        [ 2, 23],\n",
      "        [23, 23],\n",
      "        [23, 25]])\n",
      "tensor([23, 23, 25,  0])\n"
     ]
    }
   ],
   "source": [
    "print(xs_ts)\n",
    "print(ys_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x_enc = F.one_hot(xs_ts, num_classes=char_count).float()\n",
    "y_enc = F.one_hot(ys_ts, num_classes=char_count).float()\n",
    "\n",
    "print(x_enc)\n",
    "print(y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 27])\n",
      "torch.Size([4, 54])\n"
     ]
    }
   ],
   "source": [
    "print(x_enc.shape)\n",
    "x_enc_flat = x_enc.view((x_enc.size(0), -1))\n",
    "print(x_enc_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0174, 0.0244, 0.0078, 0.0303, 0.0099, 0.0992, 0.1343, 0.0068, 0.0183,\n",
      "         0.0008, 0.0175, 0.0097, 0.0048, 0.0054, 0.0155, 0.0023, 0.0011, 0.1384,\n",
      "         0.0472, 0.0266, 0.0775, 0.0043, 0.0173, 0.0640, 0.0043, 0.0027, 0.2122],\n",
      "        [0.0159, 0.2638, 0.0629, 0.0407, 0.0341, 0.0183, 0.0028, 0.0165, 0.0111,\n",
      "         0.0019, 0.0074, 0.0155, 0.0325, 0.0733, 0.0204, 0.0318, 0.0496, 0.0065,\n",
      "         0.0415, 0.0031, 0.0138, 0.1600, 0.0325, 0.0018, 0.0184, 0.0191, 0.0048],\n",
      "        [0.0235, 0.1786, 0.1370, 0.0020, 0.0672, 0.0097, 0.0093, 0.0085, 0.0394,\n",
      "         0.0121, 0.0033, 0.0068, 0.0047, 0.0010, 0.0049, 0.0090, 0.0177, 0.0152,\n",
      "         0.0531, 0.0401, 0.2340, 0.0095, 0.0107, 0.0118, 0.0718, 0.0102, 0.0092],\n",
      "        [0.1023, 0.0314, 0.1457, 0.0059, 0.0201, 0.0233, 0.0585, 0.0613, 0.0431,\n",
      "         0.0245, 0.0104, 0.0018, 0.0350, 0.0013, 0.0098, 0.0197, 0.0164, 0.0188,\n",
      "         0.0084, 0.0307, 0.2293, 0.0358, 0.0042, 0.0093, 0.0166, 0.0136, 0.0226]])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((char_count *2, char_count))\n",
    "logits = x_enc_flat @ W\n",
    "counts = torch.exp(logits)\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "print(probs)\n",
    "print(probs[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196113"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs: Final[list[Tuple[int, int]]] = []\n",
    "ys: Final[list[int]] = []\n",
    "for w in words:\n",
    "    chs: list[str] = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        xs.append((stoi[ch1], stoi[ch2]))\n",
    "        ys.append(stoi[ch3])\n",
    "xs_ts = torch.tensor(xs)\n",
    "ys_ts = torch.tensor(ys)\n",
    "n_examples = len(xs_ts)\n",
    "n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "g: torch.Generator = torch.Generator().manual_seed(2147483647)\n",
    "W: torch.Tensor = torch.randn((char_count * 2, char_count), generator=g, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 18.2454833984375\n",
      "Epoch 1: Loss: 10.044971466064453\n",
      "Epoch 2: Loss: 6.321364402770996\n",
      "Epoch 3: Loss: 4.6018595695495605\n",
      "Epoch 4: Loss: 3.7959489822387695\n",
      "Epoch 5: Loss: 3.4130616188049316\n",
      "Epoch 6: Loss: 3.228872060775757\n",
      "Epoch 7: Loss: 3.1392369270324707\n",
      "Epoch 8: Loss: 3.0951387882232666\n",
      "Epoch 9: Loss: 3.0732176303863525\n",
      "Epoch 10: Loss: 3.062211275100708\n",
      "Epoch 11: Loss: 3.0566320419311523\n",
      "Epoch 12: Loss: 3.0537774562835693\n",
      "Epoch 13: Loss: 3.0523040294647217\n",
      "Epoch 14: Loss: 3.0515365600585938\n",
      "Epoch 15: Loss: 3.051133394241333\n",
      "Epoch 16: Loss: 3.050920009613037\n",
      "Epoch 17: Loss: 3.0508060455322266\n",
      "Epoch 18: Loss: 3.0507447719573975\n",
      "Epoch 19: Loss: 3.0507116317749023\n",
      "Epoch 20: Loss: 3.0506932735443115\n",
      "Epoch 21: Loss: 3.05068302154541\n",
      "Epoch 22: Loss: 3.050677537918091\n",
      "Epoch 23: Loss: 3.0506744384765625\n",
      "Epoch 24: Loss: 3.0506725311279297\n",
      "Epoch 25: Loss: 3.0506715774536133\n",
      "Epoch 26: Loss: 3.050671339035034\n",
      "Epoch 27: Loss: 3.050671100616455\n",
      "Epoch 28: Loss: 3.050670623779297\n",
      "Epoch 29: Loss: 3.0506703853607178\n",
      "Epoch 30: Loss: 3.050670623779297\n",
      "Epoch 31: Loss: 3.0506703853607178\n",
      "Epoch 32: Loss: 3.050670623779297\n",
      "Epoch 33: Loss: 3.0506701469421387\n",
      "Epoch 34: Loss: 3.0506701469421387\n",
      "Epoch 35: Loss: 3.0506703853607178\n",
      "Epoch 36: Loss: 3.0506703853607178\n",
      "Epoch 37: Loss: 3.050670623779297\n",
      "Epoch 38: Loss: 3.0506701469421387\n",
      "Epoch 39: Loss: 3.0506701469421387\n",
      "Epoch 40: Loss: 3.0506703853607178\n",
      "Epoch 41: Loss: 3.0506701469421387\n",
      "Epoch 42: Loss: 3.0506701469421387\n",
      "Epoch 43: Loss: 3.0506701469421387\n",
      "Epoch 44: Loss: 3.0506701469421387\n",
      "Epoch 45: Loss: 3.0506701469421387\n",
      "Epoch 46: Loss: 3.0506701469421387\n",
      "Epoch 47: Loss: 3.0506701469421387\n",
      "Epoch 48: Loss: 3.0506701469421387\n",
      "Epoch 49: Loss: 3.0506701469421387\n",
      "Epoch 50: Loss: 3.0506701469421387\n",
      "Epoch 51: Loss: 3.0506701469421387\n",
      "Epoch 52: Loss: 3.0506701469421387\n",
      "Epoch 53: Loss: 3.0506701469421387\n",
      "Epoch 54: Loss: 3.0506701469421387\n",
      "Epoch 55: Loss: 3.0506701469421387\n",
      "Epoch 56: Loss: 3.0506701469421387\n",
      "Epoch 57: Loss: 3.0506701469421387\n",
      "Epoch 58: Loss: 3.0506701469421387\n",
      "Epoch 59: Loss: 3.0506701469421387\n",
      "Epoch 60: Loss: 3.0506701469421387\n",
      "Epoch 61: Loss: 3.0506701469421387\n",
      "Epoch 62: Loss: 3.0506701469421387\n",
      "Epoch 63: Loss: 3.0506701469421387\n",
      "Epoch 64: Loss: 3.0506701469421387\n",
      "Epoch 65: Loss: 3.0506701469421387\n",
      "Epoch 66: Loss: 3.0506701469421387\n",
      "Epoch 67: Loss: 3.0506701469421387\n",
      "Epoch 68: Loss: 3.0506701469421387\n",
      "Epoch 69: Loss: 3.0506701469421387\n",
      "Epoch 70: Loss: 3.0506701469421387\n",
      "Epoch 71: Loss: 3.0506701469421387\n",
      "Epoch 72: Loss: 3.0506701469421387\n",
      "Epoch 73: Loss: 3.0506701469421387\n",
      "Epoch 74: Loss: 3.0506701469421387\n",
      "Epoch 75: Loss: 3.0506701469421387\n",
      "Epoch 76: Loss: 3.0506701469421387\n",
      "Epoch 77: Loss: 3.0506701469421387\n",
      "Epoch 78: Loss: 3.0506701469421387\n",
      "Epoch 79: Loss: 3.0506701469421387\n",
      "Epoch 80: Loss: 3.0506701469421387\n",
      "Epoch 81: Loss: 3.0506701469421387\n",
      "Epoch 82: Loss: 3.0506701469421387\n",
      "Epoch 83: Loss: 3.0506701469421387\n",
      "Epoch 84: Loss: 3.0506701469421387\n",
      "Epoch 85: Loss: 3.0506701469421387\n",
      "Epoch 86: Loss: 3.0506701469421387\n",
      "Epoch 87: Loss: 3.0506701469421387\n",
      "Epoch 88: Loss: 3.0506701469421387\n",
      "Epoch 89: Loss: 3.0506701469421387\n",
      "Epoch 90: Loss: 3.0506701469421387\n",
      "Epoch 91: Loss: 3.0506701469421387\n",
      "Epoch 92: Loss: 3.0506701469421387\n",
      "Epoch 93: Loss: 3.0506701469421387\n",
      "Epoch 94: Loss: 3.0506701469421387\n",
      "Epoch 95: Loss: 3.0506701469421387\n",
      "Epoch 96: Loss: 3.0506701469421387\n",
      "Epoch 97: Loss: 3.0506701469421387\n",
      "Epoch 98: Loss: 3.0506701469421387\n",
      "Epoch 99: Loss: 3.0506701469421387\n",
      "Epoch 100: Loss: 3.0506701469421387\n",
      "Epoch 101: Loss: 3.0506701469421387\n",
      "Epoch 102: Loss: 3.0506701469421387\n",
      "Epoch 103: Loss: 3.0506701469421387\n",
      "Epoch 104: Loss: 3.0506701469421387\n",
      "Epoch 105: Loss: 3.0506701469421387\n",
      "Epoch 106: Loss: 3.0506701469421387\n",
      "Epoch 107: Loss: 3.0506701469421387\n",
      "Epoch 108: Loss: 3.0506701469421387\n",
      "Epoch 109: Loss: 3.0506701469421387\n",
      "Epoch 110: Loss: 3.0506701469421387\n",
      "Epoch 111: Loss: 3.0506701469421387\n",
      "Epoch 112: Loss: 3.0506701469421387\n",
      "Epoch 113: Loss: 3.0506701469421387\n",
      "Epoch 114: Loss: 3.0506701469421387\n",
      "Epoch 115: Loss: 3.0506701469421387\n",
      "Epoch 116: Loss: 3.0506701469421387\n",
      "Epoch 117: Loss: 3.0506701469421387\n",
      "Epoch 118: Loss: 3.0506701469421387\n",
      "Epoch 119: Loss: 3.0506701469421387\n",
      "Epoch 120: Loss: 3.0506701469421387\n",
      "Epoch 121: Loss: 3.0506701469421387\n",
      "Epoch 122: Loss: 3.0506701469421387\n",
      "Epoch 123: Loss: 3.0506701469421387\n",
      "Epoch 124: Loss: 3.0506701469421387\n",
      "Epoch 125: Loss: 3.0506701469421387\n",
      "Epoch 126: Loss: 3.0506701469421387\n",
      "Epoch 127: Loss: 3.0506701469421387\n",
      "Epoch 128: Loss: 3.0506701469421387\n",
      "Epoch 129: Loss: 3.0506701469421387\n",
      "Epoch 130: Loss: 3.0506701469421387\n",
      "Epoch 131: Loss: 3.0506701469421387\n",
      "Epoch 132: Loss: 3.0506701469421387\n",
      "Epoch 133: Loss: 3.0506701469421387\n",
      "Epoch 134: Loss: 3.0506701469421387\n",
      "Epoch 135: Loss: 3.0506701469421387\n",
      "Epoch 136: Loss: 3.0506701469421387\n",
      "Epoch 137: Loss: 3.0506701469421387\n",
      "Epoch 138: Loss: 3.0506701469421387\n",
      "Epoch 139: Loss: 3.0506701469421387\n",
      "Epoch 140: Loss: 3.0506701469421387\n",
      "Epoch 141: Loss: 3.0506701469421387\n",
      "Epoch 142: Loss: 3.0506701469421387\n",
      "Epoch 143: Loss: 3.0506701469421387\n",
      "Epoch 144: Loss: 3.0506701469421387\n",
      "Epoch 145: Loss: 3.0506701469421387\n",
      "Epoch 146: Loss: 3.0506701469421387\n",
      "Epoch 147: Loss: 3.0506701469421387\n",
      "Epoch 148: Loss: 3.0506701469421387\n",
      "Epoch 149: Loss: 3.0506701469421387\n",
      "Epoch 150: Loss: 3.0506701469421387\n",
      "Epoch 151: Loss: 3.0506701469421387\n",
      "Epoch 152: Loss: 3.0506701469421387\n",
      "Epoch 153: Loss: 3.0506701469421387\n",
      "Epoch 154: Loss: 3.0506701469421387\n",
      "Epoch 155: Loss: 3.0506701469421387\n",
      "Epoch 156: Loss: 3.0506701469421387\n",
      "Epoch 157: Loss: 3.0506701469421387\n",
      "Epoch 158: Loss: 3.0506701469421387\n",
      "Epoch 159: Loss: 3.0506701469421387\n",
      "Epoch 160: Loss: 3.0506701469421387\n",
      "Epoch 161: Loss: 3.0506701469421387\n",
      "Epoch 162: Loss: 3.0506701469421387\n",
      "Epoch 163: Loss: 3.0506701469421387\n",
      "Epoch 164: Loss: 3.0506701469421387\n",
      "Epoch 165: Loss: 3.0506701469421387\n",
      "Epoch 166: Loss: 3.0506701469421387\n",
      "Epoch 167: Loss: 3.0506701469421387\n",
      "Epoch 168: Loss: 3.0506701469421387\n",
      "Epoch 169: Loss: 3.0506701469421387\n",
      "Epoch 170: Loss: 3.0506701469421387\n",
      "Epoch 171: Loss: 3.0506701469421387\n",
      "Epoch 172: Loss: 3.0506701469421387\n",
      "Epoch 173: Loss: 3.0506701469421387\n",
      "Epoch 174: Loss: 3.0506701469421387\n",
      "Epoch 175: Loss: 3.0506701469421387\n",
      "Epoch 176: Loss: 3.0506701469421387\n",
      "Epoch 177: Loss: 3.0506701469421387\n",
      "Epoch 178: Loss: 3.0506701469421387\n",
      "Epoch 179: Loss: 3.0506701469421387\n",
      "Epoch 180: Loss: 3.0506701469421387\n",
      "Epoch 181: Loss: 3.0506701469421387\n",
      "Epoch 182: Loss: 3.0506701469421387\n",
      "Epoch 183: Loss: 3.0506701469421387\n",
      "Epoch 184: Loss: 3.0506701469421387\n",
      "Epoch 185: Loss: 3.0506701469421387\n",
      "Epoch 186: Loss: 3.0506701469421387\n",
      "Epoch 187: Loss: 3.0506701469421387\n",
      "Epoch 188: Loss: 3.0506701469421387\n",
      "Epoch 189: Loss: 3.0506701469421387\n",
      "Epoch 190: Loss: 3.0506701469421387\n",
      "Epoch 191: Loss: 3.0506701469421387\n",
      "Epoch 192: Loss: 3.0506701469421387\n",
      "Epoch 193: Loss: 3.0506701469421387\n",
      "Epoch 194: Loss: 3.0506701469421387\n",
      "Epoch 195: Loss: 3.0506701469421387\n",
      "Epoch 196: Loss: 3.0506701469421387\n",
      "Epoch 197: Loss: 3.0506701469421387\n",
      "Epoch 198: Loss: 3.0506701469421387\n",
      "Epoch 199: Loss: 3.0506701469421387\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "n_epochs: Final[int] = 200\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    x_enc = F.one_hot(xs_ts, num_classes=char_count).float()\n",
    "    x_enc_flat = x_enc.view((x_enc.size(0), -1))\n",
    "    logits = x_enc_flat @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "    loss = -torch.log(probs[torch.arange(n_examples), ys_ts]).mean() + 0.01 * (W ** 2).sum()\n",
    "    print(f\"Epoch {epoch}: Loss: {loss.item()}\")\n",
    "\n",
    "    # Backward\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    assert W.grad is not None\n",
    "    W.data -= 15.0 * (0.99 ** epoch) * W.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196113, 2])\n"
     ]
    }
   ],
   "source": [
    "print(xs_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting method.\n",
    "\n",
    "N = torch.zeros((char_count, char_count, char_count), dtype=torch.int32)\n",
    "for ch1, ch2, ch3 in zip(xs_ts[:, 0], xs_ts[:, 1], ys_ts):\n",
    "    N[ch1, ch2, ch3] += 1\n",
    "\n",
    "P = (N + 1).float()\n",
    "P /= P.sum(dim=2, keepdim=True)\n",
    "P[0, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log Likelihood: 2.0931\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in words:\n",
    "    chs: list[str] = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        prob = P[ix1, ix2, ix3]\n",
    "        log_prob = torch.log(prob)\n",
    "        log_likelihood += log_prob.item()\n",
    "        n += 1\n",
    "nll = -log_likelihood / n\n",
    "print(f\"Negative Log Likelihood: {nll:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_N = torch.zeros((char_count, char_count), dtype=torch.int32)\n",
    "for w in words:\n",
    "    chs: list[str] = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1, ix2 = stoi[ch1], stoi[ch2]\n",
    "        bigram_N[ix1, ix2] += 1\n",
    "bigram_P = (bigram_N + 1).float()\n",
    "bigram_P /= bigram_P.sum(dim=1, keepdim=True)\n",
    "bigram_P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alin.\n",
      "iahspan.\n",
      "rie.\n",
      "a.\n",
      "aiya.\n",
      "ierleige.\n",
      "indael.\n",
      "ssihailyn.\n",
      "ai.\n",
      "aylathaahemai.\n",
      "ssi.\n",
      "yna.\n",
      "arn.\n",
      "aiyitzaraelya.\n",
      "raeileis.\n",
      "a.\n",
      "al.\n",
      "wzxzair.\n",
      "een.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 1)\n",
    "for i in range(20):\n",
    "    ix1 = 0\n",
    "    ix2 = int(torch.multinomial(bigram_P[ix1], num_samples=1, replacement=True, generator=g).item())\n",
    "    out: list[str] = []\n",
    "    while True:\n",
    "        p = P[ix1, ix2]\n",
    "        ix3 = int(torch.multinomial(p, num_samples=1, replacement=True, generator=g).item())\n",
    "        out.append(itos[ix3])\n",
    "        if ix3 == 0:\n",
    "            break\n",
    "        ix1, ix2 = ix2, ix3\n",
    "    print(\"\".join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aligyyzjhspmn.\n",
      "artunrovfbvlpldttlenij.\n",
      "egdtedtesceh.\n",
      "ajyqtmi.\n",
      "lykjtbdafumaf.\n",
      "qwiwwvotfwoan.\n",
      "abyizbxggeczangdzqizeps.\n",
      "g.\n",
      "gw.\n",
      "wzxzvsxvlqmowymqfpjxgqk.\n"
     ]
    }
   ],
   "source": [
    "# Sampling from trigram NN model\n",
    "g = torch.Generator().manual_seed(2147483647 + 1)\n",
    "n_samples: Final[int] = 10\n",
    "for i in range(n_samples):\n",
    "    ix1 = 0\n",
    "    ix2 = int(torch.multinomial(bigram_P[ix1], num_samples=1, replacement=True, generator=g).item())\n",
    "    out: list[str] = []\n",
    "    while True:\n",
    "        x_enc = F.one_hot(torch.tensor([[ix1, ix2]]), num_classes=char_count).float()\n",
    "        x_enc_flat = x_enc.view((x_enc.size(0), -1))\n",
    "        logits = x_enc_flat @ W\n",
    "        counts = torch.exp(logits)\n",
    "        probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "        ix3 = int(torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item())\n",
    "        out.append(itos[ix3])\n",
    "        if ix3 == 0:\n",
    "            break\n",
    "        ix1, ix2 = ix2, ix3\n",
    "    print(\"\".join(out))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
