{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final, Tuple\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words: Final[list[str]] = open(\"names.txt\").read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N: Final[torch.Tensor] = torch.zeros((27, 27), dtype=torch.int32)\n",
    "chars: Final[list[str]] = sorted(list(set(''.join(words))))\n",
    "stoi: Final[dict[str, int]] = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos: Final[dict[int, str]] = {i: s for s, i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 32033\n",
      "Training set size: 25626\n",
      "Dev set size: 3203\n",
      "Test set size: 3204\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "# Create a PyTorch dataset from the word list\n",
    "class WordsDataset(Dataset):\n",
    "\tdef __init__(self, words_list) -> None:\n",
    "\t\tself.words = words_list\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.words)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.words[idx]\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(2147483647)\n",
    "\n",
    "# Create a proper PyTorch dataset\n",
    "dataset = WordsDataset(words)\n",
    "\n",
    "# Calculate split sizes\n",
    "n_total = len(dataset)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_dev = int(0.1 * n_total)\n",
    "n_test = n_total - n_train - n_dev  # Use the remainder to ensure the sum equals the total\n",
    "\n",
    "# Split the data\n",
    "train_dataset, dev_dataset, test_dataset = random_split(dataset, [n_train, n_dev, n_test])\n",
    "\n",
    "# Convert back to lists of words for easier processing\n",
    "train_words: list[str] = [dataset[i] for i in train_dataset.indices]\n",
    "dev_words: list[str] = [dataset[i] for i in dev_dataset.indices]\n",
    "test_words: list[str] = [dataset[i] for i in test_dataset.indices]\n",
    "\n",
    "# Print the sizes to verify\n",
    "print(f\"Total words: {n_total}\")\n",
    "print(f\"Training set size: {len(train_words)}\")\n",
    "print(f\"Dev set size: {len(dev_words)}\")\n",
    "print(f\"Test set size: {len(test_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waseem', 'zahari', 'deylin', 'thoreau', 'annalicia']\n",
      "['yuritza', 'malikye', 'lars', 'raylyn', 'ramell']\n",
      "['pieper', 'fern', 'aurora', 'jex', 'safan']\n"
     ]
    }
   ],
   "source": [
    "print(train_words[:5])\n",
    "print(dev_words[:5])\n",
    "print(test_words[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in train_words:\n",
    "    chs: list[str] = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1, ix2 = stoi[ch1], stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N + 1).float()\n",
    "P /= P.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juwe.\n",
      "ksahnaauranilevias.\n",
      "dedainrwieta.\n",
      "ssonielylarte.\n",
      "faveumerifontume.\n",
      "phynslenaruani.\n",
      "core.\n",
      "yaenon.\n",
      "ka.\n",
      "jabdinerimikimaynin.\n",
      "anaasn.\n",
      "ssorionsush.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix: int = 0\n",
    "\n",
    "for i in range(20):\n",
    "    out: list[str] = []\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ix = int(torch.multinomial(p, num_samples=1, replacement=True, generator=g).item())\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood of train set: -448245.6935\n",
      "Negative log-likelihood of train set: 2.4550\n",
      "Log-likelihood of dev set: -55712.0168\n",
      "Negative log-likelihood of dev set: 2.4546\n",
      "Log-likelihood of test set: -56145.4831\n",
      "Negative log-likelihood of test set: 2.4554\n"
     ]
    }
   ],
   "source": [
    "test_sets: list[list[str]] = [train_words, dev_words, test_words]\n",
    "set_names: list[str] = ['train', 'dev', 'test']\n",
    "\n",
    "for word_set, set_name in zip(test_sets, set_names):\n",
    "    log_likelihood: float = 0.0\n",
    "    n: int = 0\n",
    "    for w in word_set:\n",
    "        chs: list[str] = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            ix1, ix2 = stoi[ch1], stoi[ch2]\n",
    "            log_likelihood += P[ix1, ix2].log().item()\n",
    "            n += 1\n",
    "    print(f'Log-likelihood of {set_name} set: {log_likelihood:.4f}')\n",
    "    nll: float = -log_likelihood / n\n",
    "    print(f'Negative log-likelihood of {set_name} set: {nll:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_datasets(words: list[str]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "        chs: list[str] = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            xs.append(stoi[ch1])\n",
    "            ys.append(stoi[ch2])\n",
    "    return torch.tensor(xs), torch.tensor(ys)\n",
    "\n",
    "train_xs, train_ys = words_to_datasets(train_words)\n",
    "dev_xs, dev_ys = words_to_datasets(dev_words)\n",
    "test_xs, test_ys = words_to_datasets(test_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss: 2.7190\n",
      "Epoch 20, loss: 2.5890\n",
      "Epoch 30, loss: 2.5444\n",
      "Epoch 40, loss: 2.5230\n",
      "Epoch 50, loss: 2.5110\n",
      "Epoch 60, loss: 2.5035\n",
      "Epoch 70, loss: 2.4985\n",
      "Epoch 80, loss: 2.4950\n",
      "Epoch 90, loss: 2.4924\n",
      "Epoch 100, loss: 2.4904\n",
      "Epoch 110, loss: 2.4889\n",
      "Epoch 120, loss: 2.4877\n",
      "Epoch 130, loss: 2.4867\n",
      "Epoch 140, loss: 2.4859\n",
      "Epoch 150, loss: 2.4853\n",
      "Epoch 160, loss: 2.4848\n",
      "Epoch 170, loss: 2.4843\n",
      "Epoch 180, loss: 2.4839\n",
      "Epoch 190, loss: 2.4836\n",
      "Epoch 200, loss: 2.4833\n",
      "Training set NLL: 2.4656\n",
      "Dev set NLL: 2.4647\n",
      "Test set NLL: 2.4654\n"
     ]
    }
   ],
   "source": [
    "g: torch.Generator = torch.Generator().manual_seed(2147483647)\n",
    "W: torch.Tensor = torch.randn((len(stoi), len(stoi)), generator=g, requires_grad=True)\n",
    "n_examples: int = train_xs.nelement()\n",
    "reg_strength: float = 0.01\n",
    "\n",
    "n_epochs: int = 200\n",
    "for epoch in range(n_epochs):\n",
    "    assert isinstance(train_xs, torch.Tensor)\n",
    "    # Directly use indexing into W instead of one-hot encoding\n",
    "    logits = W[train_xs]\n",
    "\n",
    "    # Skip the one-hot encoding code that follows\n",
    "    x_enc = F.one_hot(train_xs, num_classes=len(stoi)).float()\n",
    "    logits = x_enc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "    loss = -torch.log(probs[torch.arange(n_examples), train_ys]).mean() + reg_strength * W.pow(2).mean()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, loss: {loss.item():.4f}')\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    assert W.grad is not None\n",
    "    W.data += -50 * W.grad\n",
    "\n",
    "def calculate_nll(xs: torch.Tensor, ys: torch.Tensor) -> float:\n",
    "    x_enc = F.one_hot(xs, num_classes=len(stoi)).float()\n",
    "    logits = x_enc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "    return -torch.log(probs[torch.arange(xs.nelement()), ys]).mean().item()\n",
    "\n",
    "train_nll: float = calculate_nll(train_xs, train_ys)\n",
    "dev_nll: float = calculate_nll(dev_xs, dev_ys)\n",
    "test_nll: float = calculate_nll(test_xs, test_ys)\n",
    "print(f'Training set NLL: {train_nll:.4f}')\n",
    "print(f'Dev set NLL: {dev_nll:.4f}')\n",
    "print(f'Test set NLL: {test_nll:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juwe.\n",
      "ksahnaauranilevias.\n"
     ]
    }
   ],
   "source": [
    "g: Final[torch.Generator] = torch.Generator().manual_seed(2147483647)\n",
    "n_samples: Final[int] = 10\n",
    "\n",
    "for i in range(n_samples):\n",
    "    out: list[str] = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        x_enc = F.one_hot(torch.tensor([ix]), num_classes=len(stoi)).float()\n",
    "        logits = x_enc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[int(ix)])\n",
    "        if ix == 0:\n",
    "            break;\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set NLL: 2.4656\n",
      "Dev set NLL: 2.4647\n",
      "Test set NLL: 2.4654\n"
     ]
    }
   ],
   "source": [
    "def calculate_nll(xs: torch.Tensor, ys: torch.Tensor) -> float:\n",
    "    x_enc = F.one_hot(xs, num_classes=len(stoi)).float()\n",
    "    logits = x_enc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "    return -torch.log(probs[torch.arange(xs.nelement()), ys]).mean().item()\n",
    "\n",
    "train_nll: float = calculate_nll(train_xs, train_ys)\n",
    "dev_nll: float = calculate_nll(dev_xs, dev_ys)\n",
    "test_nll: float = calculate_nll(test_xs, test_ys)\n",
    "print(f'Training set NLL: {train_nll:.4f}')\n",
    "print(f'Dev set NLL: {dev_nll:.4f}')\n",
    "print(f'Test set NLL: {test_nll:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement trigram using counting\n",
    "\n",
    "char_count: int = len(stoi)\n",
    "N = torch.zeros((char_count, char_count, char_count), dtype=torch.int32)\n",
    "for w in train_words:\n",
    "    chs: list[str] = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1\n",
    "\n",
    "P = (N + 1).float()\n",
    "P /= P.sum(dim=2, keepdim=True)\n",
    "P[0, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set NLL (trigram): 2.0954\n",
      "Dev set NLL (trigram): 2.1310\n",
      "Test set NLL (trigram): 2.1213\n"
     ]
    }
   ],
   "source": [
    "def get_nll_trigram(test_words: list[str]) -> float:\n",
    "    ll: float = 0.0\n",
    "    n: int = 0\n",
    "    for w in test_words:\n",
    "        chs: list[str] = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "            ll += P[ix1, ix2, ix3].log().item()\n",
    "            n += 1\n",
    "    return -ll / n\n",
    "print(f'Training set NLL (trigram): {get_nll_trigram(train_words):.4f}')\n",
    "print(f'Dev set NLL (trigram): {get_nll_trigram(dev_words):.4f}')\n",
    "print(f'Test set NLL (trigram): {get_nll_trigram(test_words):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
