{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final, Tuple\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words: Final[list[str]] = open(\"names.txt\").read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N: Final[torch.Tensor] = torch.zeros((27, 27), dtype=torch.int32)\n",
    "chars: Final[list[str]] = sorted(list(set(''.join(words))))\n",
    "stoi: Final[dict[str, int]] = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos: Final[dict[int, str]] = {i: s for s, i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 32033\n",
      "Training set size: 25626\n",
      "Dev set size: 3203\n",
      "Test set size: 3204\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "# Create a PyTorch dataset from the word list\n",
    "class WordsDataset(Dataset):\n",
    "\tdef __init__(self, words_list) -> None:\n",
    "\t\tself.words = words_list\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.words)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.words[idx]\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(2147483647)\n",
    "\n",
    "# Create a proper PyTorch dataset\n",
    "dataset = WordsDataset(words)\n",
    "\n",
    "# Calculate split sizes\n",
    "n_total = len(dataset)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_dev = int(0.1 * n_total)\n",
    "n_test = n_total - n_train - n_dev  # Use the remainder to ensure the sum equals the total\n",
    "\n",
    "# Split the data\n",
    "train_dataset, dev_dataset, test_dataset = random_split(dataset, [n_train, n_dev, n_test])\n",
    "\n",
    "# Convert back to lists of words for easier processing\n",
    "train_words: list[str] = [dataset[i] for i in train_dataset.indices]\n",
    "dev_words: list[str] = [dataset[i] for i in dev_dataset.indices]\n",
    "test_words: list[str] = [dataset[i] for i in test_dataset.indices]\n",
    "\n",
    "# Print the sizes to verify\n",
    "print(f\"Total words: {n_total}\")\n",
    "print(f\"Training set size: {len(train_words)}\")\n",
    "print(f\"Dev set size: {len(dev_words)}\")\n",
    "print(f\"Test set size: {len(test_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in train_words:\n",
    "    chs: list[str] = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1, ix2 = stoi[ch1], stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N + 1).float()\n",
    "P /= P.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juwe.\n",
      "ksahnaauranilevias.\n",
      "dedainrwieta.\n",
      "ssonielylarte.\n",
      "faveumerifontume.\n",
      "phynslenaruani.\n",
      "core.\n",
      "yaenon.\n",
      "ka.\n",
      "jabdinerimikimaynin.\n",
      "anaasn.\n",
      "ssorionsush.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix: int = 0\n",
    "\n",
    "for i in range(20):\n",
    "    out: list[str] = []\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ix = int(torch.multinomial(p, num_samples=1, replacement=True, generator=g).item())\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood of train set: -448245.6935\n",
      "Negative log-likelihood of train set: 2.4550\n",
      "Log-likelihood of dev set: -55712.0168\n",
      "Negative log-likelihood of dev set: 2.4546\n",
      "Log-likelihood of test set: -56145.4831\n",
      "Negative log-likelihood of test set: 2.4554\n"
     ]
    }
   ],
   "source": [
    "test_sets: list[list[str]] = [train_words, dev_words, test_words]\n",
    "set_names: list[str] = ['train', 'dev', 'test']\n",
    "\n",
    "for word_set, set_name in zip(test_sets, set_names):\n",
    "    log_likelihood: float = 0.0\n",
    "    n: int = 0\n",
    "    for w in word_set:\n",
    "        chs: list[str] = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            ix1, ix2 = stoi[ch1], stoi[ch2]\n",
    "            log_likelihood += P[ix1, ix2].log().item()\n",
    "            n += 1\n",
    "    print(f'Log-likelihood of {set_name} set: {log_likelihood:.4f}')\n",
    "    nll: float = -log_likelihood / n\n",
    "    print(f'Negative log-likelihood of {set_name} set: {nll:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_datasets(words: list[str]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "        chs: list[str] = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            xs.append(stoi[ch1])\n",
    "            ys.append(stoi[ch2])\n",
    "    return torch.tensor(xs), torch.tensor(ys)\n",
    "\n",
    "train_xs, train_ys = words_to_datasets(train_words)\n",
    "dev_xs, dev_ys = words_to_datasets(dev_words)\n",
    "test_xs, test_ys = words_to_datasets(test_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "g: torch.Generator = torch.Generator().manual_seed(2147483647)\n",
    "W: torch.Tensor = torch.randn((len(stoi), len(stoi)), generator=g, requires_grad=True)\n",
    "n_examples: int = train_xs.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 3.7646\n",
      "Epoch 2, loss: 3.3761\n",
      "Epoch 3, loss: 3.1591\n",
      "Epoch 4, loss: 3.0258\n",
      "Epoch 5, loss: 2.9335\n",
      "Epoch 6, loss: 2.8666\n",
      "Epoch 7, loss: 2.8163\n",
      "Epoch 8, loss: 2.7770\n",
      "Epoch 9, loss: 2.7453\n",
      "Epoch 10, loss: 2.7190\n",
      "Epoch 11, loss: 2.6967\n",
      "Epoch 12, loss: 2.6776\n",
      "Epoch 13, loss: 2.6611\n",
      "Epoch 14, loss: 2.6467\n",
      "Epoch 15, loss: 2.6340\n",
      "Epoch 16, loss: 2.6228\n",
      "Epoch 17, loss: 2.6129\n",
      "Epoch 18, loss: 2.6040\n",
      "Epoch 19, loss: 2.5961\n",
      "Epoch 20, loss: 2.5890\n",
      "Epoch 21, loss: 2.5825\n",
      "Epoch 22, loss: 2.5767\n",
      "Epoch 23, loss: 2.5714\n",
      "Epoch 24, loss: 2.5665\n",
      "Epoch 25, loss: 2.5620\n",
      "Epoch 26, loss: 2.5579\n",
      "Epoch 27, loss: 2.5542\n",
      "Epoch 28, loss: 2.5506\n",
      "Epoch 29, loss: 2.5474\n",
      "Epoch 30, loss: 2.5444\n",
      "Epoch 31, loss: 2.5415\n",
      "Epoch 32, loss: 2.5389\n",
      "Epoch 33, loss: 2.5365\n",
      "Epoch 34, loss: 2.5341\n",
      "Epoch 35, loss: 2.5320\n",
      "Epoch 36, loss: 2.5300\n",
      "Epoch 37, loss: 2.5281\n",
      "Epoch 38, loss: 2.5263\n",
      "Epoch 39, loss: 2.5246\n",
      "Epoch 40, loss: 2.5230\n",
      "Epoch 41, loss: 2.5215\n",
      "Epoch 42, loss: 2.5200\n",
      "Epoch 43, loss: 2.5187\n",
      "Epoch 44, loss: 2.5174\n",
      "Epoch 45, loss: 2.5162\n",
      "Epoch 46, loss: 2.5150\n",
      "Epoch 47, loss: 2.5140\n",
      "Epoch 48, loss: 2.5129\n",
      "Epoch 49, loss: 2.5119\n",
      "Epoch 50, loss: 2.5110\n",
      "Epoch 51, loss: 2.5101\n",
      "Epoch 52, loss: 2.5092\n",
      "Epoch 53, loss: 2.5084\n",
      "Epoch 54, loss: 2.5076\n",
      "Epoch 55, loss: 2.5069\n",
      "Epoch 56, loss: 2.5061\n",
      "Epoch 57, loss: 2.5054\n",
      "Epoch 58, loss: 2.5048\n",
      "Epoch 59, loss: 2.5041\n",
      "Epoch 60, loss: 2.5035\n",
      "Epoch 61, loss: 2.5029\n",
      "Epoch 62, loss: 2.5024\n",
      "Epoch 63, loss: 2.5018\n",
      "Epoch 64, loss: 2.5013\n",
      "Epoch 65, loss: 2.5008\n",
      "Epoch 66, loss: 2.5003\n",
      "Epoch 67, loss: 2.4999\n",
      "Epoch 68, loss: 2.4994\n",
      "Epoch 69, loss: 2.4990\n",
      "Epoch 70, loss: 2.4985\n",
      "Epoch 71, loss: 2.4981\n",
      "Epoch 72, loss: 2.4977\n",
      "Epoch 73, loss: 2.4974\n",
      "Epoch 74, loss: 2.4970\n",
      "Epoch 75, loss: 2.4966\n",
      "Epoch 76, loss: 2.4963\n",
      "Epoch 77, loss: 2.4960\n",
      "Epoch 78, loss: 2.4956\n",
      "Epoch 79, loss: 2.4953\n",
      "Epoch 80, loss: 2.4950\n",
      "Epoch 81, loss: 2.4947\n",
      "Epoch 82, loss: 2.4944\n",
      "Epoch 83, loss: 2.4941\n",
      "Epoch 84, loss: 2.4939\n",
      "Epoch 85, loss: 2.4936\n",
      "Epoch 86, loss: 2.4934\n",
      "Epoch 87, loss: 2.4931\n",
      "Epoch 88, loss: 2.4929\n",
      "Epoch 89, loss: 2.4926\n",
      "Epoch 90, loss: 2.4924\n",
      "Epoch 91, loss: 2.4922\n",
      "Epoch 92, loss: 2.4920\n",
      "Epoch 93, loss: 2.4918\n",
      "Epoch 94, loss: 2.4915\n",
      "Epoch 95, loss: 2.4913\n",
      "Epoch 96, loss: 2.4912\n",
      "Epoch 97, loss: 2.4910\n",
      "Epoch 98, loss: 2.4908\n",
      "Epoch 99, loss: 2.4906\n",
      "Epoch 100, loss: 2.4904\n",
      "Epoch 101, loss: 2.4903\n",
      "Epoch 102, loss: 2.4901\n",
      "Epoch 103, loss: 2.4899\n",
      "Epoch 104, loss: 2.4898\n",
      "Epoch 105, loss: 2.4896\n",
      "Epoch 106, loss: 2.4895\n",
      "Epoch 107, loss: 2.4893\n",
      "Epoch 108, loss: 2.4892\n",
      "Epoch 109, loss: 2.4890\n",
      "Epoch 110, loss: 2.4889\n",
      "Epoch 111, loss: 2.4888\n",
      "Epoch 112, loss: 2.4886\n",
      "Epoch 113, loss: 2.4885\n",
      "Epoch 114, loss: 2.4884\n",
      "Epoch 115, loss: 2.4883\n",
      "Epoch 116, loss: 2.4881\n",
      "Epoch 117, loss: 2.4880\n",
      "Epoch 118, loss: 2.4879\n",
      "Epoch 119, loss: 2.4878\n",
      "Epoch 120, loss: 2.4877\n",
      "Epoch 121, loss: 2.4876\n",
      "Epoch 122, loss: 2.4875\n",
      "Epoch 123, loss: 2.4874\n",
      "Epoch 124, loss: 2.4873\n",
      "Epoch 125, loss: 2.4872\n",
      "Epoch 126, loss: 2.4871\n",
      "Epoch 127, loss: 2.4870\n",
      "Epoch 128, loss: 2.4869\n",
      "Epoch 129, loss: 2.4868\n",
      "Epoch 130, loss: 2.4867\n",
      "Epoch 131, loss: 2.4866\n",
      "Epoch 132, loss: 2.4865\n",
      "Epoch 133, loss: 2.4865\n",
      "Epoch 134, loss: 2.4864\n",
      "Epoch 135, loss: 2.4863\n",
      "Epoch 136, loss: 2.4862\n",
      "Epoch 137, loss: 2.4862\n",
      "Epoch 138, loss: 2.4861\n",
      "Epoch 139, loss: 2.4860\n",
      "Epoch 140, loss: 2.4859\n",
      "Epoch 141, loss: 2.4859\n",
      "Epoch 142, loss: 2.4858\n",
      "Epoch 143, loss: 2.4857\n",
      "Epoch 144, loss: 2.4857\n",
      "Epoch 145, loss: 2.4856\n",
      "Epoch 146, loss: 2.4855\n",
      "Epoch 147, loss: 2.4855\n",
      "Epoch 148, loss: 2.4854\n",
      "Epoch 149, loss: 2.4853\n",
      "Epoch 150, loss: 2.4853\n",
      "Epoch 151, loss: 2.4852\n",
      "Epoch 152, loss: 2.4852\n",
      "Epoch 153, loss: 2.4851\n",
      "Epoch 154, loss: 2.4851\n",
      "Epoch 155, loss: 2.4850\n",
      "Epoch 156, loss: 2.4850\n",
      "Epoch 157, loss: 2.4849\n",
      "Epoch 158, loss: 2.4849\n",
      "Epoch 159, loss: 2.4848\n",
      "Epoch 160, loss: 2.4848\n",
      "Epoch 161, loss: 2.4847\n",
      "Epoch 162, loss: 2.4847\n",
      "Epoch 163, loss: 2.4846\n",
      "Epoch 164, loss: 2.4846\n",
      "Epoch 165, loss: 2.4845\n",
      "Epoch 166, loss: 2.4845\n",
      "Epoch 167, loss: 2.4844\n",
      "Epoch 168, loss: 2.4844\n",
      "Epoch 169, loss: 2.4843\n",
      "Epoch 170, loss: 2.4843\n",
      "Epoch 171, loss: 2.4843\n",
      "Epoch 172, loss: 2.4842\n",
      "Epoch 173, loss: 2.4842\n",
      "Epoch 174, loss: 2.4841\n",
      "Epoch 175, loss: 2.4841\n",
      "Epoch 176, loss: 2.4841\n",
      "Epoch 177, loss: 2.4840\n",
      "Epoch 178, loss: 2.4840\n",
      "Epoch 179, loss: 2.4840\n",
      "Epoch 180, loss: 2.4839\n",
      "Epoch 181, loss: 2.4839\n",
      "Epoch 182, loss: 2.4839\n",
      "Epoch 183, loss: 2.4838\n",
      "Epoch 184, loss: 2.4838\n",
      "Epoch 185, loss: 2.4838\n",
      "Epoch 186, loss: 2.4837\n",
      "Epoch 187, loss: 2.4837\n",
      "Epoch 188, loss: 2.4837\n",
      "Epoch 189, loss: 2.4836\n",
      "Epoch 190, loss: 2.4836\n",
      "Epoch 191, loss: 2.4836\n",
      "Epoch 192, loss: 2.4835\n",
      "Epoch 193, loss: 2.4835\n",
      "Epoch 194, loss: 2.4835\n",
      "Epoch 195, loss: 2.4835\n",
      "Epoch 196, loss: 2.4834\n",
      "Epoch 197, loss: 2.4834\n",
      "Epoch 198, loss: 2.4834\n",
      "Epoch 199, loss: 2.4833\n",
      "Epoch 200, loss: 2.4833\n"
     ]
    }
   ],
   "source": [
    "n_epochs: int = 200\n",
    "for epoch in range(n_epochs):\n",
    "    assert isinstance(train_xs, torch.Tensor)\n",
    "    x_enc = F.one_hot(train_xs, num_classes=len(stoi)).float()\n",
    "    logits = x_enc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "    loss = -torch.log(probs[torch.arange(n_examples), train_ys]).mean() + 0.01 * W.pow(2).mean()\n",
    "    print(f'Epoch {epoch + 1}, loss: {loss.item():.4f}')\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    assert W.grad is not None\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juwe.\n",
      "ksahnaauranilevias.\n"
     ]
    }
   ],
   "source": [
    "g: Final[torch.Generator] = torch.Generator().manual_seed(2147483647)\n",
    "n_samples: Final[int] = 10\n",
    "\n",
    "for i in range(n_samples):\n",
    "    out: list[str] = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        x_enc = F.one_hot(torch.tensor([ix]), num_classes=len(stoi)).float()\n",
    "        logits = x_enc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[int(ix)])\n",
    "        if ix == 0:\n",
    "            break;\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set NLL: 2.4656\n",
      "Dev set NLL: 2.4647\n",
      "Test set NLL: 2.4654\n"
     ]
    }
   ],
   "source": [
    "def calculate_nll(xs: torch.Tensor, ys: torch.Tensor) -> float:\n",
    "    x_enc = F.one_hot(xs, num_classes=len(stoi)).float()\n",
    "    logits = x_enc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "    return -torch.log(probs[torch.arange(xs.nelement()), ys]).mean().item()\n",
    "\n",
    "train_nll: float = calculate_nll(train_xs, train_ys)\n",
    "dev_nll: float = calculate_nll(dev_xs, dev_ys)\n",
    "test_nll: float = calculate_nll(test_xs, test_ys)\n",
    "print(f'Training set NLL: {train_nll:.4f}')\n",
    "print(f'Dev set NLL: {dev_nll:.4f}')\n",
    "print(f'Test set NLL: {test_nll:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement trigram using counting\n",
    "\n",
    "char_count: int = len(stoi)\n",
    "N = torch.zeros((char_count, char_count, char_count), dtype=torch.int32)\n",
    "for w in train_words:\n",
    "    chs: list[str] = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1\n",
    "\n",
    "P = (N + 1).float()\n",
    "P /= P.sum(dim=2, keepdim=True)\n",
    "P[0, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set NLL (trigram): 2.0954\n",
      "Dev set NLL (trigram): 2.1310\n",
      "Test set NLL (trigram): 2.1213\n"
     ]
    }
   ],
   "source": [
    "def get_nll_trigram(test_words: list[str]) -> float:\n",
    "    ll: float = 0.0\n",
    "    n: int = 0\n",
    "    for w in test_words:\n",
    "        chs: list[str] = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1, ix2, ix3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "            ll += P[ix1, ix2, ix3].log().item()\n",
    "            n += 1\n",
    "    return -ll / n\n",
    "print(f'Training set NLL (trigram): {get_nll_trigram(train_words):.4f}')\n",
    "print(f'Dev set NLL (trigram): {get_nll_trigram(dev_words):.4f}')\n",
    "print(f'Test set NLL (trigram): {get_nll_trigram(test_words):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
